{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ef4f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFull definition of a GPT Language Model, all of it in this single file.\\nReferences:\\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\\n2) huggingface/transformers PyTorch implementation:\\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995564c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d67417d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Ci'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a897941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2764010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(text))))\n",
    "stoi = {c : i for i, c in enumerate(chars)}\n",
    "itos = {i : c for c, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da33f612",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n_train = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n_train]       # 90%\n",
    "eval_data = data[n_train:]         # 10%\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633ff87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# block_size = 32\n",
    "# batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "# epochs = 1000\n",
    "# eval_interval = 100\n",
    "# learning_rate = 1e-3\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feda93f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x131afafcf30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d02bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate small batch of data of inputs data X and targets y\n",
    "    data = train_data if split == 'train' else eval_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#     X = torch.stack([data[i:i+block_size] for i in ix])\n",
    "#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    X = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        X, y = X.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58317582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = get_batch('eval')\n",
    "# X[:2], y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd59e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, y = get_batch(split)\n",
    "            logits, loss = model(X, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbcf0e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "#eval_interval = 2000\n",
    "eval_interval = 100\n",
    "log_interval = 100\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "#dataset = 'openwebtext'\n",
    "#gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "gradient_accumulation_steps = 3 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "#block_size = 1024\n",
    "block_size = 32\n",
    "# model\n",
    "# n_layer = 12\n",
    "# n_head = 12\n",
    "# n_embd = 768\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 64\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# admw optimizer\n",
    "#learning_rate = 6e-4 # max leanring rate\n",
    "#epochs = 600000 # total number of training iterations\n",
    "learning_rate = 1e-3 # max leanring rate\n",
    "epochs = 1000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "#warmup_iters = 2000 # how many steps to warm up for\n",
    "#lr_decay_iters = 600000 # should be ~= epochs per Chinchilla\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "lr_decay_iters = epochs # should be ~= epochs per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl' 'glob', etc\n",
    "# system\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16' or 'float16', the latter will auto implement a GradScaler\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "#--------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7277fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias, PyTorch doesn't support simply bias=False\"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return F.layer_norm(X, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias) # 3 means key,query,value concatenate\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            #causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                         .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        B, T, C = X.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(X).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) @ (B, nh, hs, T) -> (B, nhs, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # output prjection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias) # in transformer paper, the dimension is 512 and projectino to 2048, so it's 4 times\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn. Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # projection the 4 times dimension back to dimension\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.c_fc(X)\n",
    "        X = self.gelu(X)\n",
    "        X = self.c_proj(X)\n",
    "        X = self.dropout(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X + self.attn(self.ln_1(X)) # + means residual connection\n",
    "        X = X + self.mlp(self.ln_2(X)) # + means residual connection\n",
    "        return X\n",
    "\n",
    "\n",
    "class NanoGPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_embedding = nn.Embedding(config.vocab_size, config.n_embd), # (vocab_size, C)\n",
    "            position_embedding = nn.Embedding(config.block_size, config.n_embd), # (T, C)\n",
    "            dropout = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias), # final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.token_embedding.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply sepcial scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        \n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get substracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.position_embedding.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, X, y=None):\n",
    "        device = X.device\n",
    "        # X and y are both (B, T) tensor integers, B = batch_size, T = block_size\n",
    "        B, T = X.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape (T)\n",
    "        \n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.token_embedding(X) # (B, T, C)\n",
    "        pos_emb = self.transformer.position_embedding(pos) # (T, C)\n",
    "        X = self.transformer.dropout(tok_emb + pos_emb)\n",
    "        for block in self.transformer.blocks:\n",
    "            X = block(X) # (B, T, C)\n",
    "        X = self.transformer.ln_f(X)   # (B, T, C)\n",
    "        \n",
    "        if y is not None:\n",
    "            # if we are given some desired y also calculate the loss\n",
    "            logits = self.lm_head(X) # (B, T, vocab_size)\n",
    "#             B, T, C = logits.shape\n",
    "#             logits = logits.view(B*T, C)\n",
    "#             y = y.view(B*T)\n",
    "#             loss = F.cross_entropy(logits, y)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference time mini optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(X[:, [-1], :]) # note: using list[-1] to preserve the time dim\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.position_embedding.weight = nn.Parameter(self.transformer.position_embedding.weight[:block_size])\n",
    "        for block in self.transformer.blocks:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pertrained gpt: %s\" % model_type)\n",
    "        \n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':        dict(n_layer=12, n_head=12, n_embd=768), # 124M parameters\n",
    "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024), # 350M parameters\n",
    "            'gpt2-large':  dict(n_layer=36, n_head=20, n_embd=1280), # 774M parameters\n",
    "            'gpt2-x1':     dict(n_layer=48, n_head=15, n_embd=1600), # 1558M parameters\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "        \n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        \n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf) != {len(sd_keys)}}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # speecial treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model\n",
    "    \n",
    "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all the condidata parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorm don't\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer\n",
    "    \n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd // cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def generate(self, X, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices X (LongTensor of shape (B, T)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        # X is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            # crop X to the last block_size tokens\n",
    "            X_cond = X if X.size(1) <= self.config.block_size else X[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, loss = self(X_cond)\n",
    "            # focus only on the last time step, pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature # becomes (B, C)\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_nxt = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            X = torch.cat((X, idx_nxt), dim=1) # (B, T+1)\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51353f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NanoGPTModel(GPTConfig())\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee55d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('total parameters number:', sum(p.nelement() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74e4f188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # train model\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# for iter in range(epochs):\n",
    "#     # every once in a while evaluate the loss on train and eval sets\n",
    "#     if iter % eval_interval == 0:\n",
    "#         losses = estimate_loss()\n",
    "#         print(f\"step {iter}: train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "    \n",
    "#     # sample a batch of data\n",
    "#     Xb, yb = get_batch('train')\n",
    "    \n",
    "#     # evaluate the loss\n",
    "#     logits, loss = model(Xb, yb)\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab9f02c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be 1,152\n"
     ]
    }
   ],
   "source": [
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iteration per process proportinally\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be {tokens_per_iter:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a13e6fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a99dcc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c35de1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.20M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NanoGPTModel(\n",
       "  (transformer): ModuleDict(\n",
       "    (token_embedding): Embedding(65, 64)\n",
       "    (position_embedding): Embedding(32, 64)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (c_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (gelu): GELU(approximate=none)\n",
       "          (c_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (c_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (gelu): GELU(approximate=none)\n",
       "          (c_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (c_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (gelu): GELU(approximate=none)\n",
       "          (c_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=64, out_features=192, bias=False)\n",
       "          (c_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (gelu): GELU(approximate=none)\n",
       "          (c_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=64, out_features=65, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                 bias=bias, vocab_size=vocab_size, dropout=dropout) # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab_size we'll use for from-scratch training\n",
    "    #print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    model = NanoGPTModel(GPTConfig(**model_args))\n",
    "elif init_from == 'resume':\n",
    "    print(\"Resuming training from {out_dir}\")\n",
    "    # resume training from a checkpoint\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    # create the model\n",
    "    model = NanoGPTModel(GPTConfig(**model_args))\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary\n",
    "    # honestly no idea how checkpoints sometimes get this prefix. have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = NanoGPTModel.from_pretrained(init_from, override_args)\n",
    "    # read off the created config params, so we can store them into checkpoint correctly\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "397c68cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=32, vocab_size=65, n_layer=4, n_head=4, n_embd=64, dropout=0.0, bias=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c337ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaoeason\\AppData\\Local\\anaconda3\\envs\\disk\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ef89518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 18, with 202,816 parameters\n",
      "num non-decayed parameter tensors: 9, with 576 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = model.configure_optimizer(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c7f0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (take a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch >= 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81a5f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap model into DDP container\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdd8f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78b8d310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1928, eval loss 4.1889\n",
      "iter 0: loss 4.1919, time 34847.90ms, mfu -100.00%\n",
      "step 100: train loss 2.8043, eval loss 2.8131\n",
      "saving checkpoint to out\n",
      "iter 100: loss 2.8130, time 35714.68ms, mfu 0.00%\n",
      "step 200: train loss 2.4909, eval loss 2.4926\n",
      "saving checkpoint to out\n",
      "iter 200: loss 2.5632, time 36679.51ms, mfu 0.00%\n",
      "step 300: train loss 2.4079, eval loss 2.4145\n",
      "saving checkpoint to out\n",
      "iter 300: loss 2.3739, time 37334.45ms, mfu 0.00%\n",
      "step 400: train loss 2.3489, eval loss 2.3415\n",
      "saving checkpoint to out\n",
      "iter 400: loss 2.3289, time 35853.41ms, mfu 0.00%\n",
      "step 500: train loss 2.2840, eval loss 2.2981\n",
      "saving checkpoint to out\n",
      "iter 500: loss 2.3265, time 36980.11ms, mfu 0.00%\n",
      "step 600: train loss 2.2383, eval loss 2.2547\n",
      "saving checkpoint to out\n",
      "iter 600: loss 2.3080, time 37726.17ms, mfu 0.00%\n",
      "step 700: train loss 2.2061, eval loss 2.2128\n",
      "saving checkpoint to out\n",
      "iter 700: loss 2.2699, time 37347.96ms, mfu 0.00%\n",
      "step 800: train loss 2.1737, eval loss 2.1859\n",
      "saving checkpoint to out\n",
      "iter 800: loss 2.1499, time 37293.99ms, mfu 0.00%\n",
      "step 900: train loss 2.1622, eval loss 2.1869\n",
      "iter 900: loss 2.1202, time 37257.60ms, mfu 0.00%\n",
      "step 1000: train loss 2.1514, eval loss 2.1618\n",
      "saving checkpoint to out\n",
      "iter 1000: loss 2.1627, time 36830.29ms, mfu 0.00%\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "X, y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of the process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # evaluate the loss on train/eval sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter:\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"eval/loss\": losses['eval'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['eval'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['eval']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "    \n",
    "    # forward backward update. with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradient at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, y = get_batch('train')\n",
    "        # backward pass, with gradient sacling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "    \n",
    "    # termination condition\n",
    "    if iter_num > epochs:\n",
    "        break\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47817bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CARE:\n",
      "And I thy wathe thak'l wid that onds of and the friore; ith held thre mour weart brangesert to whall bede bows me that my ban hord\n",
      "Ang is berosoutiminn, to art o my bladde,\n",
      "Sough douene wertr, mord ton you,\n",
      "And fard thearest wol blotssen, you,\n",
      "An me them broth marknseseeles is him ar fror ies to thes.\n",
      "\n",
      "WOFongs, mord, she II serim tho he sount,\n",
      "Start amy thand mall or shy we houghalle this dou, surest you\n",
      "Th to dime tint and hend fearsse miort\n",
      "Bund her osof ba my ach brave imert anne ict brie, in forst a my.\n",
      "\n",
      "WOLORGLER:\n",
      "Hads trein thy, heils heave illlf han an otees if hernctesie for har the and by in sour, wigh should. I dover bookes wices and thet ind's mest o angeang fors, well mims on sticck,\n",
      "And thesurt st hould thou if tar sat, dases.\n",
      "\n",
      "TUTRIUSS:\n",
      "INo my sim whay, hilll we of hert.\n",
      "\n",
      "ACEERIAND-\n",
      "AOLOO:\n",
      "T:\n",
      "Therit iler if sor the meses sus savar ast ame, to must o his, shell won mat.\n",
      "\n",
      "ANESS:\n",
      "Bad I as thy livel.\n",
      "\n",
      "ARK:\n",
      "Nos here, and divinng tho as this for shate stas,\n",
      "Hat having:\n",
      "Th that it sunt, sans berourts of had deprovigher tim off tict siet horde\n",
      "And them seand splakess he theak.\n",
      "\n",
      "\n",
      "SRK:\n",
      "Serdfor'es an to dot thake if art heee\n",
      "He and sipllt to ont at bragientes.\n",
      "\n",
      "WORURELO:\n",
      "Band to hind mity tin ouncence.\n",
      "\n",
      "KILALIE say, wirll hof her blovere. yan\n",
      "\n",
      "CANENTO:\n",
      "An, murd thisst off or fowng theror fut and marssis, and hent ins tey tinkit, ing wong and shirs the the andsered ifort;\n",
      "Inds wor amps yourst mor of, soret the,\n",
      "Muth duch spriss ot ind wis a deashes thit. heat.\n",
      "\n",
      "\n",
      "\n",
      "GHKIUCEOUES:\n",
      "WA:\n",
      "Ses or tay it it sof beng oongis, whir of a mangond ourr breansing,\n",
      "And houg whe frrear, ist a mut fith\n",
      "Sane trir angaiety sthe berer if sthares\n",
      "Woull\n",
      "He tof ang shucch thim tesor onde mire a fling seeself ay stlee ives hend beat a to tourgar of-dneth thath t wim a andes sall thare.\n",
      "BERII:\n",
      "Hed, that dou by wordse to b things.\n",
      "\n",
      "HORDUD:\n",
      "Hut he hand that say ave, sthrrat.\n",
      "\n",
      "ANGENES:\n",
      "Warte mors to hom as of told thes welld,\n",
      "Shan of trour sor ant thay me but thas avous the swerray the wit\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000, temperature=1.0, top_k=10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "827061d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved checkpoint\n",
    "checkpoint = torch.load(os.path.join(out_dir, 'ckpt.pt'), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eae6858c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_layer': 4,\n",
       " 'n_head': 4,\n",
       " 'n_embd': 64,\n",
       " 'block_size': 32,\n",
       " 'bias': False,\n",
       " 'vocab_size': 65,\n",
       " 'dropout': 0.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0af5ac4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1618)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['best_val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0472256b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['iter_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15a2dc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-7.7831e-04, -1.4290e-03, -1.2733e-03,  ...,  1.3082e-03,\n",
       "             5.9090e-04,  6.4573e-04],\n",
       "           [ 3.0794e-03,  3.8900e-03,  1.8857e-03,  ..., -1.9322e-02,\n",
       "            -7.4273e-03,  4.6929e-03],\n",
       "           [ 2.8753e-04,  1.0117e-03,  7.6174e-04,  ..., -5.8909e-04,\n",
       "            -5.1775e-04, -5.7067e-04],\n",
       "           ...,\n",
       "           [-1.0056e-05, -2.3573e-04,  6.0078e-05,  ...,  3.3780e-04,\n",
       "             8.3510e-05, -1.7949e-04],\n",
       "           [ 1.6744e-03,  4.6966e-05, -1.6250e-03,  ...,  2.1410e-03,\n",
       "            -9.3943e-04,  2.0704e-05],\n",
       "           [-7.4394e-05,  3.5052e-04, -4.1024e-05,  ...,  4.4126e-05,\n",
       "             1.7401e-05,  2.5365e-04]]),\n",
       "   'exp_avg_sq': tensor([[3.8345e-05, 5.3719e-05, 2.9915e-05,  ..., 3.8629e-05, 7.4341e-05,\n",
       "            3.3832e-05],\n",
       "           [1.7261e-04, 3.3286e-04, 1.3393e-04,  ..., 3.1803e-03, 4.3525e-04,\n",
       "            3.7568e-04],\n",
       "           [8.0293e-06, 1.0669e-05, 1.5047e-05,  ..., 1.8907e-05, 6.2861e-06,\n",
       "            1.3432e-05],\n",
       "           ...,\n",
       "           [8.7564e-07, 2.0918e-06, 2.4602e-06,  ..., 8.3999e-07, 2.9035e-06,\n",
       "            3.3803e-06],\n",
       "           [4.3085e-05, 2.7469e-05, 2.3126e-05,  ..., 5.3845e-05, 1.9457e-05,\n",
       "            2.8630e-05],\n",
       "           [2.9663e-07, 3.1967e-06, 5.7458e-07,  ..., 2.6967e-07, 3.5124e-07,\n",
       "            2.8663e-06]])},\n",
       "  1: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[ 1.0259e-03, -1.9676e-04, -2.1090e-04,  ...,  3.2902e-05,\n",
       "             1.4145e-05, -2.4754e-04],\n",
       "           [-2.6124e-04, -1.2053e-03, -2.4656e-03,  ..., -4.2246e-04,\n",
       "             4.6501e-04,  8.6061e-04],\n",
       "           [-5.8740e-04,  6.2374e-04, -1.4506e-03,  ..., -1.5004e-03,\n",
       "            -2.0048e-03,  1.9549e-03],\n",
       "           ...,\n",
       "           [ 3.3742e-04, -2.0651e-04,  1.0474e-03,  ...,  5.2238e-03,\n",
       "             5.2937e-04, -3.6764e-03],\n",
       "           [-1.0361e-03,  1.6829e-03, -6.4260e-04,  ...,  8.0849e-04,\n",
       "             1.1215e-03,  1.9542e-04],\n",
       "           [-4.9538e-04,  2.0414e-03, -1.6351e-03,  ...,  1.1947e-03,\n",
       "             1.8678e-03,  8.0631e-04]]),\n",
       "   'exp_avg_sq': tensor([[3.8244e-05, 1.2292e-04, 3.5328e-05,  ..., 1.9972e-05, 7.1407e-05,\n",
       "            5.2770e-05],\n",
       "           [5.2689e-05, 5.0562e-05, 3.6107e-05,  ..., 4.7148e-05, 8.0054e-05,\n",
       "            3.6615e-05],\n",
       "           [4.2904e-05, 5.0876e-05, 3.0985e-05,  ..., 5.0112e-05, 6.9432e-05,\n",
       "            3.7566e-05],\n",
       "           ...,\n",
       "           [2.7662e-05, 3.2002e-05, 2.8932e-05,  ..., 8.1349e-05, 6.4013e-05,\n",
       "            3.7123e-05],\n",
       "           [2.5223e-05, 4.3741e-05, 3.0745e-05,  ..., 5.3463e-05, 4.6695e-05,\n",
       "            2.2249e-05],\n",
       "           [2.0677e-05, 2.3460e-05, 2.5420e-05,  ..., 5.9013e-05, 5.0579e-05,\n",
       "            1.0839e-05]])},\n",
       "  2: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[ 4.6903e-04,  2.5266e-04, -4.0059e-04,  ..., -4.4386e-04,\n",
       "            -3.7647e-04,  6.2686e-05],\n",
       "           [ 9.1796e-05,  3.0522e-04, -2.8921e-04,  ..., -2.1697e-04,\n",
       "            -1.4067e-04,  2.0866e-04],\n",
       "           [-1.7639e-04, -4.7256e-05,  2.8683e-04,  ...,  8.0465e-05,\n",
       "             1.1588e-04, -1.8771e-04],\n",
       "           ...,\n",
       "           [ 5.2992e-05, -3.6448e-04,  9.9395e-07,  ...,  7.3939e-05,\n",
       "             1.6279e-04, -5.6267e-04],\n",
       "           [-2.5186e-04, -5.6606e-04,  1.1528e-04,  ...,  1.7175e-04,\n",
       "             6.9467e-04,  2.9257e-04],\n",
       "           [-8.1625e-05, -1.0855e-04,  1.2570e-03,  ...,  9.6752e-04,\n",
       "             8.4805e-04, -1.2059e-03]]),\n",
       "   'exp_avg_sq': tensor([[2.1830e-06, 4.6007e-06, 1.1344e-06,  ..., 2.3688e-06, 2.9752e-06,\n",
       "            8.7482e-07],\n",
       "           [4.0017e-07, 7.3748e-07, 3.4195e-07,  ..., 5.3025e-07, 4.0232e-07,\n",
       "            4.3624e-07],\n",
       "           [3.5493e-07, 3.2453e-07, 2.7659e-07,  ..., 4.5900e-07, 4.7940e-07,\n",
       "            2.9870e-07],\n",
       "           ...,\n",
       "           [7.0030e-06, 9.3887e-06, 1.9980e-05,  ..., 7.3048e-06, 5.4470e-06,\n",
       "            1.7064e-05],\n",
       "           [1.4130e-05, 1.1804e-05, 3.3846e-05,  ..., 1.3447e-05, 1.0144e-05,\n",
       "            2.5859e-05],\n",
       "           [9.4828e-06, 8.4927e-06, 3.4566e-05,  ..., 1.3750e-05, 8.1765e-06,\n",
       "            2.5589e-05]])},\n",
       "  3: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-1.7383e-03,  6.1932e-05, -9.7812e-05,  ..., -1.8993e-03,\n",
       "            -1.4991e-03, -1.8144e-04],\n",
       "           [-2.4533e-04, -3.3957e-05,  2.4407e-04,  ..., -5.1002e-04,\n",
       "             2.4109e-04,  2.0061e-04],\n",
       "           [-8.4709e-04, -2.0700e-04,  3.7764e-04,  ..., -1.3086e-03,\n",
       "            -6.2314e-04, -2.9334e-04],\n",
       "           ...,\n",
       "           [ 2.3101e-03, -3.7386e-04, -1.0101e-03,  ...,  1.5309e-03,\n",
       "             2.0386e-05,  6.7017e-04],\n",
       "           [ 1.7511e-03,  9.4234e-04, -4.0489e-04,  ...,  2.8622e-03,\n",
       "             1.7109e-03, -3.4000e-04],\n",
       "           [-3.0040e-04,  2.6557e-05, -1.3547e-04,  ...,  1.9128e-04,\n",
       "             6.0138e-05, -5.4484e-05]]),\n",
       "   'exp_avg_sq': tensor([[2.2893e-05, 3.1875e-06, 1.8141e-05,  ..., 3.3320e-05, 1.1239e-05,\n",
       "            8.2937e-06],\n",
       "           [1.3218e-05, 2.2767e-06, 1.3108e-05,  ..., 1.3670e-05, 7.3969e-06,\n",
       "            4.7380e-06],\n",
       "           [1.2985e-05, 2.9815e-06, 1.2382e-05,  ..., 2.3432e-05, 1.1367e-05,\n",
       "            5.0313e-06],\n",
       "           ...,\n",
       "           [2.2044e-05, 2.7886e-06, 1.4805e-05,  ..., 2.2713e-05, 1.2607e-05,\n",
       "            6.1307e-06],\n",
       "           [3.1168e-05, 7.3337e-06, 1.9761e-05,  ..., 5.3736e-05, 3.1999e-05,\n",
       "            8.2523e-06],\n",
       "           [7.9678e-06, 1.4535e-06, 6.5537e-06,  ..., 9.2243e-06, 5.0892e-06,\n",
       "            1.7188e-06]])},\n",
       "  4: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[ 1.1595e-05,  1.4676e-03, -4.7982e-04,  ..., -9.0036e-04,\n",
       "            -1.0028e-04,  8.9455e-04],\n",
       "           [-1.4254e-04, -2.7882e-04,  1.7548e-04,  ..., -5.1705e-04,\n",
       "             1.9393e-04, -2.5872e-05],\n",
       "           [-5.0555e-04, -9.0246e-04,  4.1935e-04,  ...,  9.6597e-04,\n",
       "             1.8054e-04, -5.1898e-04],\n",
       "           ...,\n",
       "           [-3.8748e-06,  3.8499e-04, -9.5631e-05,  ..., -2.6700e-05,\n",
       "             3.8215e-04,  6.2401e-04],\n",
       "           [-3.9213e-04,  1.2250e-03, -3.3590e-04,  ..., -1.5569e-04,\n",
       "             2.0538e-04,  4.6834e-04],\n",
       "           [-4.6678e-05,  3.1488e-04,  7.3366e-05,  ..., -4.5099e-04,\n",
       "            -2.5043e-04,  4.4004e-04]]),\n",
       "   'exp_avg_sq': tensor([[4.3655e-06, 1.3080e-05, 2.7370e-06,  ..., 4.9228e-06, 1.9362e-06,\n",
       "            3.8982e-06],\n",
       "           [5.2635e-06, 4.4076e-06, 3.6033e-06,  ..., 6.1544e-06, 6.9601e-06,\n",
       "            3.0259e-06],\n",
       "           [3.1479e-06, 6.6326e-06, 3.0394e-06,  ..., 6.1585e-06, 3.0662e-06,\n",
       "            4.4578e-06],\n",
       "           ...,\n",
       "           [3.2248e-06, 3.7672e-06, 1.9934e-06,  ..., 3.2215e-06, 2.3332e-06,\n",
       "            2.8325e-06],\n",
       "           [6.0856e-06, 2.3154e-05, 2.8869e-06,  ..., 3.2149e-06, 3.8266e-06,\n",
       "            6.4689e-06],\n",
       "           [1.5397e-06, 1.4521e-06, 1.3889e-06,  ..., 3.2839e-06, 8.3138e-07,\n",
       "            1.3812e-06]])},\n",
       "  5: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[ 2.8645e-05,  3.1383e-04, -1.8402e-04,  ...,  6.7450e-04,\n",
       "            -4.2978e-04,  2.1169e-04],\n",
       "           [ 2.7004e-04, -1.2329e-03, -1.0280e-04,  ...,  1.0002e-03,\n",
       "            -4.3720e-05,  7.2232e-04],\n",
       "           [-5.7151e-04,  5.2365e-04, -4.3924e-04,  ...,  1.0626e-03,\n",
       "            -4.9064e-04,  7.1369e-04],\n",
       "           ...,\n",
       "           [-9.5030e-05,  9.8997e-04,  3.9847e-04,  ..., -5.0069e-03,\n",
       "            -7.9903e-04, -3.3993e-03],\n",
       "           [-1.9436e-04, -2.7541e-04, -1.1284e-04,  ..., -4.1715e-04,\n",
       "             1.7357e-04, -2.2912e-04],\n",
       "           [ 3.0909e-04, -2.8403e-04, -6.1178e-05,  ...,  3.7187e-04,\n",
       "             2.9133e-04,  2.1667e-04]]),\n",
       "   'exp_avg_sq': tensor([[3.0145e-06, 7.2339e-06, 3.0447e-06,  ..., 7.6825e-06, 3.9505e-06,\n",
       "            3.7222e-06],\n",
       "           [4.8319e-06, 1.0013e-05, 3.0409e-06,  ..., 1.3998e-05, 4.8106e-06,\n",
       "            6.0055e-06],\n",
       "           [2.4565e-06, 1.0147e-05, 2.6943e-06,  ..., 9.2779e-06, 3.9160e-06,\n",
       "            4.2366e-06],\n",
       "           ...,\n",
       "           [5.1937e-06, 1.6897e-05, 8.1344e-06,  ..., 2.1116e-04, 8.7734e-06,\n",
       "            1.0335e-04],\n",
       "           [2.3528e-06, 8.3087e-06, 3.4093e-06,  ..., 1.4178e-05, 5.0028e-06,\n",
       "            8.4304e-06],\n",
       "           [1.7801e-06, 4.5636e-06, 1.5812e-06,  ..., 2.7547e-06, 1.9927e-06,\n",
       "            1.7743e-06]])},\n",
       "  6: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-1.3519e-04,  7.3645e-05,  2.1832e-05,  ..., -3.0853e-04,\n",
       "             3.6321e-05,  5.1509e-06],\n",
       "           [-9.2253e-05, -1.0314e-04,  4.8192e-05,  ..., -1.0105e-04,\n",
       "             2.0917e-05, -8.9767e-05],\n",
       "           [ 7.7078e-05, -7.3593e-05,  7.3096e-06,  ...,  2.7392e-04,\n",
       "            -2.8171e-05, -1.8191e-05],\n",
       "           ...,\n",
       "           [ 1.4062e-04,  5.0241e-05,  6.3789e-05,  ..., -5.5034e-04,\n",
       "             1.1471e-03, -3.4637e-04],\n",
       "           [ 2.6713e-04, -2.5278e-04, -6.9691e-05,  ...,  4.8423e-04,\n",
       "             2.1360e-04, -2.9069e-04],\n",
       "           [ 1.9672e-04,  1.9169e-04,  1.2364e-04,  ...,  9.0665e-05,\n",
       "            -7.6436e-04,  3.0159e-04]]),\n",
       "   'exp_avg_sq': tensor([[2.1169e-07, 4.6066e-07, 2.0194e-07,  ..., 2.8005e-07, 1.7837e-07,\n",
       "            1.0549e-07],\n",
       "           [5.8243e-08, 1.5378e-07, 9.0418e-08,  ..., 1.1099e-07, 7.5086e-08,\n",
       "            6.5035e-08],\n",
       "           [1.3577e-07, 2.5390e-07, 1.3885e-07,  ..., 1.9196e-07, 1.2708e-07,\n",
       "            9.5755e-08],\n",
       "           ...,\n",
       "           [2.6738e-06, 7.7047e-06, 1.8201e-06,  ..., 2.4117e-05, 6.1848e-06,\n",
       "            4.6530e-06],\n",
       "           [9.0606e-07, 1.8433e-06, 8.0162e-07,  ..., 9.4162e-06, 2.5568e-06,\n",
       "            1.4187e-06],\n",
       "           [2.5390e-06, 4.1717e-06, 8.8931e-07,  ..., 2.7452e-05, 5.3770e-06,\n",
       "            3.2386e-06]])},\n",
       "  7: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-7.0828e-04,  1.0389e-03, -2.9081e-05,  ...,  6.4973e-04,\n",
       "             1.1117e-03,  3.4078e-04],\n",
       "           [-2.6005e-04,  2.0686e-04,  3.0456e-04,  ...,  3.5605e-04,\n",
       "             1.1654e-04, -1.0935e-04],\n",
       "           [-3.8523e-05,  6.8012e-04,  3.3934e-04,  ...,  1.1567e-04,\n",
       "             3.6663e-04,  4.7463e-04],\n",
       "           ...,\n",
       "           [-1.5913e-04,  5.7962e-04, -2.8824e-05,  ..., -2.0178e-04,\n",
       "             7.1280e-04,  8.1527e-04],\n",
       "           [ 4.8116e-04, -5.2780e-04, -6.0907e-05,  ..., -7.8722e-04,\n",
       "            -1.4349e-03, -3.4156e-04],\n",
       "           [-2.0808e-04,  1.1283e-04, -3.9706e-05,  ...,  2.1154e-04,\n",
       "             2.3544e-04, -6.7918e-05]]),\n",
       "   'exp_avg_sq': tensor([[7.1437e-06, 9.2493e-06, 4.6359e-06,  ..., 9.6277e-06, 1.2546e-05,\n",
       "            1.0244e-05],\n",
       "           [6.1069e-06, 7.6308e-06, 3.9500e-06,  ..., 1.2158e-05, 6.6530e-06,\n",
       "            1.2651e-05],\n",
       "           [1.7458e-06, 3.1759e-06, 1.9967e-06,  ..., 5.3932e-06, 5.2643e-06,\n",
       "            3.0159e-06],\n",
       "           ...,\n",
       "           [2.5876e-06, 4.0682e-06, 2.9125e-06,  ..., 6.0717e-06, 8.2281e-06,\n",
       "            7.4301e-06],\n",
       "           [4.7204e-06, 6.7198e-06, 2.7724e-06,  ..., 1.2784e-05, 1.2283e-05,\n",
       "            1.0706e-05],\n",
       "           [3.1062e-06, 4.6501e-06, 2.1526e-06,  ..., 5.3006e-06, 4.9032e-06,\n",
       "            3.7736e-06]])},\n",
       "  8: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-8.6280e-05,  3.5664e-04, -1.2076e-04,  ..., -3.6922e-04,\n",
       "            -1.6702e-04,  3.3279e-04],\n",
       "           [ 4.2255e-05, -1.4032e-03, -8.9629e-04,  ...,  1.1011e-03,\n",
       "             1.4233e-04, -1.0056e-04],\n",
       "           [-1.7120e-04, -1.6092e-04, -1.0519e-04,  ..., -1.1422e-04,\n",
       "             2.5642e-06, -6.8213e-05],\n",
       "           ...,\n",
       "           [-6.7589e-05,  7.7646e-04,  6.2019e-04,  ..., -2.2256e-04,\n",
       "             1.2353e-04,  4.6147e-04],\n",
       "           [-1.6221e-04,  1.2814e-04, -3.1244e-04,  ..., -1.6184e-05,\n",
       "             2.3902e-05,  1.3351e-04],\n",
       "           [-2.3479e-04,  3.3169e-05, -1.3877e-04,  ..., -2.2393e-04,\n",
       "             1.2008e-05, -1.5469e-05]]),\n",
       "   'exp_avg_sq': tensor([[1.3029e-06, 1.9993e-06, 1.0732e-06,  ..., 2.3485e-06, 1.3132e-06,\n",
       "            1.3235e-06],\n",
       "           [8.6667e-06, 2.0379e-05, 1.3843e-05,  ..., 1.4287e-05, 4.7873e-06,\n",
       "            1.0267e-05],\n",
       "           [5.9298e-07, 1.0545e-06, 4.6099e-07,  ..., 9.5684e-07, 3.3030e-07,\n",
       "            4.5445e-07],\n",
       "           ...,\n",
       "           [2.6769e-06, 3.2537e-06, 3.0271e-06,  ..., 5.3150e-06, 2.1127e-06,\n",
       "            2.6462e-06],\n",
       "           [3.6562e-07, 7.6822e-07, 6.1915e-07,  ..., 1.2795e-06, 3.6953e-07,\n",
       "            4.7608e-07],\n",
       "           [5.0138e-07, 1.5424e-06, 7.7975e-07,  ..., 1.5499e-06, 5.7487e-07,\n",
       "            6.4775e-07]])},\n",
       "  9: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-1.6217e-04,  8.4635e-04,  4.5988e-05,  ...,  3.7376e-04,\n",
       "            -4.0063e-04,  4.9669e-05],\n",
       "           [-9.4186e-05, -1.8681e-04, -1.0830e-04,  ..., -8.0463e-04,\n",
       "             3.5047e-04, -5.6946e-05],\n",
       "           [-7.2146e-04,  2.6091e-03,  7.2696e-05,  ...,  7.3254e-04,\n",
       "             1.6098e-04, -1.1945e-04],\n",
       "           ...,\n",
       "           [ 4.7478e-04, -4.1349e-04, -8.3107e-05,  ...,  2.4111e-04,\n",
       "             2.3615e-04,  8.5124e-05],\n",
       "           [ 3.1215e-04,  1.0581e-04,  1.8424e-04,  ...,  1.6465e-04,\n",
       "             3.1476e-04,  8.8642e-05],\n",
       "           [ 1.7671e-04, -1.9565e-03, -5.7256e-05,  ...,  6.6924e-05,\n",
       "             4.2427e-05,  1.3957e-05]]),\n",
       "   'exp_avg_sq': tensor([[2.9419e-06, 4.2303e-05, 1.3239e-06,  ..., 1.1755e-05, 1.1030e-06,\n",
       "            9.7260e-07],\n",
       "           [3.4318e-06, 4.1826e-05, 6.9696e-07,  ..., 8.4695e-06, 1.3218e-06,\n",
       "            1.5263e-06],\n",
       "           [1.0024e-05, 6.2893e-05, 1.6038e-06,  ..., 1.1429e-05, 9.0492e-07,\n",
       "            1.4254e-06],\n",
       "           ...,\n",
       "           [5.3470e-06, 3.4703e-05, 1.3854e-06,  ..., 1.3772e-05, 9.2317e-07,\n",
       "            6.4179e-07],\n",
       "           [6.1187e-06, 3.1774e-05, 1.5803e-06,  ..., 1.4491e-05, 1.6351e-06,\n",
       "            2.2280e-06],\n",
       "           [3.5547e-06, 4.2825e-05, 1.1256e-06,  ..., 1.2620e-05, 1.1618e-06,\n",
       "            1.0581e-06]])},\n",
       "  10: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-1.0375e-04, -1.1710e-04,  7.9909e-05,  ..., -3.4272e-05,\n",
       "            -5.0738e-05, -9.2224e-05],\n",
       "           [-6.0732e-05, -1.3927e-04,  1.0234e-04,  ..., -8.4095e-05,\n",
       "            -6.9725e-05, -1.1724e-04],\n",
       "           [ 8.8676e-05,  5.9110e-05, -4.9769e-05,  ...,  6.8518e-05,\n",
       "            -1.3212e-05,  2.3362e-05],\n",
       "           ...,\n",
       "           [ 3.0562e-04,  8.3978e-04,  1.4675e-04,  ..., -1.0427e-03,\n",
       "             1.8211e-05,  4.5216e-04],\n",
       "           [ 4.3125e-04,  5.6707e-04, -2.1798e-05,  ...,  2.3802e-04,\n",
       "             3.2604e-04,  3.0285e-04],\n",
       "           [-6.1263e-05,  2.0948e-04, -7.9954e-05,  ..., -3.0900e-04,\n",
       "            -2.9115e-04,  6.3844e-06]]),\n",
       "   'exp_avg_sq': tensor([[9.7880e-08, 1.2323e-07, 8.8244e-08,  ..., 5.5384e-08, 6.5084e-08,\n",
       "            6.1275e-08],\n",
       "           [1.2766e-07, 2.2016e-07, 1.2561e-07,  ..., 1.0862e-07, 7.4765e-08,\n",
       "            1.0212e-07],\n",
       "           [1.0017e-07, 1.6113e-07, 6.7049e-08,  ..., 8.4914e-08, 6.4924e-08,\n",
       "            5.7268e-08],\n",
       "           ...,\n",
       "           [3.1053e-06, 6.2512e-06, 3.4730e-06,  ..., 5.2864e-06, 1.1403e-06,\n",
       "            3.0698e-06],\n",
       "           [2.9837e-06, 5.4119e-06, 4.5696e-06,  ..., 6.0512e-06, 1.8511e-06,\n",
       "            3.4867e-06],\n",
       "           [1.3983e-06, 2.9136e-06, 2.2716e-06,  ..., 3.7980e-06, 5.2971e-07,\n",
       "            1.6306e-06]])},\n",
       "  11: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[ 8.0411e-04, -2.9753e-04,  1.0207e-04,  ...,  1.9359e-05,\n",
       "             8.5589e-04,  3.4407e-05],\n",
       "           [ 3.0019e-04, -2.4620e-04,  1.1834e-04,  ..., -9.4646e-05,\n",
       "            -6.8910e-05, -2.1964e-04],\n",
       "           [ 8.3584e-04, -5.9776e-05, -1.1718e-03,  ..., -6.2938e-04,\n",
       "             2.3553e-04, -2.0092e-04],\n",
       "           ...,\n",
       "           [ 1.7845e-04, -1.3958e-04, -3.8123e-04,  ..., -5.5909e-04,\n",
       "            -3.9014e-04, -6.3720e-04],\n",
       "           [-3.3249e-04, -1.6198e-04,  5.5238e-04,  ..., -4.0413e-04,\n",
       "             1.6833e-04, -4.3319e-04],\n",
       "           [-7.5995e-04, -1.2255e-04,  5.9647e-04,  ...,  4.8643e-05,\n",
       "             3.2213e-05, -1.9964e-04]]),\n",
       "   'exp_avg_sq': tensor([[4.6497e-06, 1.9183e-06, 3.3487e-06,  ..., 4.3512e-06, 4.2637e-06,\n",
       "            1.7561e-06],\n",
       "           [2.8995e-06, 1.7876e-06, 2.9589e-06,  ..., 6.8449e-06, 3.6559e-06,\n",
       "            1.3979e-06],\n",
       "           [7.1788e-06, 1.2050e-06, 7.3293e-06,  ..., 5.1131e-06, 4.3441e-06,\n",
       "            1.3082e-06],\n",
       "           ...,\n",
       "           [4.7566e-06, 1.7588e-06, 3.4912e-06,  ..., 5.6828e-06, 4.8426e-06,\n",
       "            2.3648e-06],\n",
       "           [4.0064e-06, 1.2737e-06, 3.0878e-06,  ..., 4.0303e-06, 3.6485e-06,\n",
       "            1.5029e-06],\n",
       "           [4.3740e-06, 2.1872e-06, 3.1824e-06,  ..., 7.3231e-06, 4.1990e-06,\n",
       "            1.8178e-06]])},\n",
       "  12: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-2.1440e-04,  9.7201e-05, -2.0190e-05,  ...,  1.8506e-04,\n",
       "            -3.3574e-05, -5.1296e-05],\n",
       "           [ 1.1899e-04,  6.1526e-05,  8.8567e-05,  ..., -1.0098e-04,\n",
       "            -1.6179e-04, -1.0599e-04],\n",
       "           [-3.7726e-04, -4.4334e-04,  1.6815e-04,  ...,  1.4021e-04,\n",
       "             6.3967e-05, -2.2551e-04],\n",
       "           ...,\n",
       "           [ 3.7898e-04, -1.8403e-04, -5.7536e-04,  ..., -8.2573e-04,\n",
       "             3.5148e-04,  3.9459e-04],\n",
       "           [-1.5050e-04, -6.3165e-04,  2.7450e-04,  ...,  3.4429e-04,\n",
       "             2.4334e-04,  8.4044e-05],\n",
       "           [ 2.3489e-05,  4.8416e-05, -1.7590e-04,  ..., -5.2736e-05,\n",
       "            -1.8630e-04, -2.2213e-05]]),\n",
       "   'exp_avg_sq': tensor([[8.9306e-07, 1.4700e-06, 6.8627e-07,  ..., 1.4017e-06, 6.0256e-07,\n",
       "            8.4835e-07],\n",
       "           [2.6053e-07, 2.9781e-07, 1.9006e-07,  ..., 2.2065e-07, 1.7628e-07,\n",
       "            1.3949e-07],\n",
       "           [5.1365e-07, 1.1609e-06, 4.9433e-07,  ..., 1.0537e-06, 5.1926e-07,\n",
       "            7.1600e-07],\n",
       "           ...,\n",
       "           [2.4335e-06, 4.4363e-06, 3.3388e-06,  ..., 3.5943e-06, 1.6519e-06,\n",
       "            2.9932e-06],\n",
       "           [4.0465e-06, 2.8170e-06, 1.4830e-06,  ..., 2.6346e-06, 1.2156e-06,\n",
       "            1.4286e-06],\n",
       "           [2.9175e-07, 3.5695e-07, 3.9305e-07,  ..., 3.2054e-07, 2.2240e-07,\n",
       "            1.7175e-07]])},\n",
       "  13: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-6.0279e-05, -3.7287e-05,  8.2449e-05,  ..., -1.9791e-05,\n",
       "             2.2253e-04, -1.2435e-04],\n",
       "           [-1.0045e-04, -1.9026e-04, -4.6768e-04,  ..., -6.7239e-04,\n",
       "            -2.3319e-04,  1.5478e-04],\n",
       "           [-3.2503e-04,  1.5961e-04,  1.3976e-04,  ...,  7.8733e-04,\n",
       "            -3.3197e-04,  2.5386e-05],\n",
       "           ...,\n",
       "           [-7.9306e-05,  1.3514e-04, -1.6422e-04,  ...,  4.0774e-04,\n",
       "            -2.4471e-04,  8.4936e-05],\n",
       "           [ 2.8836e-04,  9.6068e-05, -5.4611e-04,  ..., -5.1131e-04,\n",
       "            -3.7905e-05,  4.5620e-05],\n",
       "           [ 3.3454e-04, -5.4795e-06, -3.7099e-04,  ..., -1.8825e-03,\n",
       "            -5.2965e-05,  8.6577e-05]]),\n",
       "   'exp_avg_sq': tensor([[1.7411e-06, 1.9942e-07, 1.8577e-06,  ..., 1.4003e-05, 2.6920e-06,\n",
       "            4.4233e-07],\n",
       "           [3.2986e-06, 4.1955e-07, 2.2558e-06,  ..., 1.7496e-05, 5.0169e-06,\n",
       "            5.5367e-07],\n",
       "           [1.3529e-06, 1.7735e-07, 1.3019e-06,  ..., 9.7787e-06, 3.1910e-06,\n",
       "            4.4869e-07],\n",
       "           ...,\n",
       "           [1.8308e-06, 2.1841e-07, 9.7741e-07,  ..., 9.0831e-06, 3.3183e-06,\n",
       "            3.3757e-07],\n",
       "           [1.7040e-06, 2.7672e-07, 3.8070e-06,  ..., 1.4635e-05, 2.7878e-06,\n",
       "            4.5286e-07],\n",
       "           [3.0378e-06, 4.6066e-07, 2.3174e-06,  ..., 1.9901e-05, 4.5406e-06,\n",
       "            6.4013e-07]])},\n",
       "  14: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-3.7825e-05,  2.1230e-06,  1.1207e-04,  ...,  7.2850e-05,\n",
       "            -1.5907e-05, -9.1661e-06],\n",
       "           [-5.7373e-05, -9.3494e-05,  1.2538e-04,  ...,  1.4401e-06,\n",
       "            -2.1658e-07, -6.4270e-05],\n",
       "           [ 1.2677e-05, -2.7151e-05, -8.2772e-05,  ..., -1.3152e-05,\n",
       "             2.0134e-05,  1.7821e-05],\n",
       "           ...,\n",
       "           [-2.9932e-04,  1.7961e-04, -1.2539e-05,  ...,  1.1795e-05,\n",
       "             1.1653e-04, -3.3174e-05],\n",
       "           [ 1.4679e-04,  6.0019e-05, -1.0679e-04,  ...,  2.3972e-04,\n",
       "             1.5609e-04,  6.6086e-05],\n",
       "           [-2.1828e-04,  8.0798e-05,  1.8156e-04,  ..., -1.2768e-05,\n",
       "             2.4562e-04, -1.8827e-04]]),\n",
       "   'exp_avg_sq': tensor([[9.9786e-08, 1.5022e-07, 8.0592e-08,  ..., 5.7575e-08, 5.2870e-08,\n",
       "            7.6736e-08],\n",
       "           [4.1787e-08, 6.4242e-08, 4.3055e-08,  ..., 5.3887e-08, 5.2412e-08,\n",
       "            4.5840e-08],\n",
       "           [4.0098e-08, 7.0961e-08, 4.0016e-08,  ..., 5.6964e-08, 3.8026e-08,\n",
       "            2.8248e-08],\n",
       "           ...,\n",
       "           [1.0209e-06, 2.7018e-06, 5.8154e-07,  ..., 3.9369e-06, 5.2195e-07,\n",
       "            1.3155e-06],\n",
       "           [7.5615e-07, 1.9929e-06, 4.0429e-07,  ..., 3.9892e-06, 4.5818e-07,\n",
       "            9.7673e-07],\n",
       "           [7.9349e-07, 2.0691e-06, 8.4020e-07,  ..., 4.0191e-06, 6.4186e-07,\n",
       "            9.8306e-07]])},\n",
       "  15: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[ 3.8692e-04,  7.2309e-04, -4.0742e-04,  ..., -8.0235e-05,\n",
       "            -7.2848e-05,  1.1124e-04],\n",
       "           [-4.9667e-04, -5.8522e-04,  4.3240e-04,  ...,  1.9058e-04,\n",
       "             1.0730e-04, -4.9971e-04],\n",
       "           [-4.3613e-04, -3.0062e-04,  6.8331e-04,  ..., -9.7337e-04,\n",
       "            -2.4072e-04,  8.6739e-05],\n",
       "           ...,\n",
       "           [-2.8104e-04, -2.0003e-04,  3.8656e-04,  ...,  5.3824e-04,\n",
       "             2.8729e-04, -4.1525e-04],\n",
       "           [-5.0127e-05, -7.9118e-04,  5.3086e-05,  ...,  4.0539e-06,\n",
       "             1.3727e-04, -1.7363e-04],\n",
       "           [ 2.3283e-04,  4.6228e-05, -5.8625e-04,  ...,  8.2054e-04,\n",
       "             3.4280e-04, -3.0434e-04]]),\n",
       "   'exp_avg_sq': tensor([[4.0101e-06, 6.3483e-06, 1.1659e-05,  ..., 8.7388e-06, 1.9890e-06,\n",
       "            2.9228e-06],\n",
       "           [6.7513e-06, 1.5808e-05, 1.9268e-05,  ..., 8.0679e-06, 2.1449e-06,\n",
       "            4.7251e-06],\n",
       "           [6.2686e-06, 6.3640e-06, 1.2916e-05,  ..., 1.0488e-05, 2.2803e-06,\n",
       "            2.1569e-06],\n",
       "           ...,\n",
       "           [1.9527e-06, 3.7358e-06, 5.4626e-06,  ..., 4.5549e-06, 1.6227e-06,\n",
       "            1.8124e-06],\n",
       "           [4.3882e-06, 6.0150e-06, 1.0196e-05,  ..., 5.8285e-06, 2.2103e-06,\n",
       "            2.1729e-06],\n",
       "           [4.6880e-06, 8.7228e-06, 1.2859e-05,  ..., 1.0202e-05, 2.1329e-06,\n",
       "            2.3461e-06]])},\n",
       "  16: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[-1.1750e-04, -6.7631e-05, -5.7702e-05,  ...,  7.2934e-05,\n",
       "             9.1278e-05, -4.0970e-05],\n",
       "           [-5.8157e-05, -1.9823e-05, -3.9915e-05,  ..., -4.5284e-05,\n",
       "             2.6907e-05,  4.0832e-05],\n",
       "           [ 1.6095e-04,  1.3011e-04, -1.3079e-04,  ..., -9.1554e-05,\n",
       "            -1.3234e-04,  2.7600e-06],\n",
       "           ...,\n",
       "           [-1.6397e-04,  2.1984e-05,  7.4510e-05,  ...,  6.5940e-05,\n",
       "             8.1489e-05,  6.9895e-06],\n",
       "           [-6.5159e-04,  2.3311e-04,  7.7053e-05,  ...,  6.3713e-04,\n",
       "             4.9731e-04, -1.0697e-04],\n",
       "           [-5.2386e-04, -1.5653e-04, -6.4062e-05,  ...,  5.3352e-04,\n",
       "             6.4912e-04,  5.5481e-04]]),\n",
       "   'exp_avg_sq': tensor([[6.6252e-07, 6.6529e-07, 3.9733e-07,  ..., 5.9272e-07, 3.7265e-07,\n",
       "            6.5517e-07],\n",
       "           [2.2847e-07, 3.1531e-07, 1.2175e-07,  ..., 2.5712e-07, 1.1522e-07,\n",
       "            2.0066e-07],\n",
       "           [2.8118e-07, 3.2543e-07, 1.6747e-07,  ..., 4.2143e-07, 1.8555e-07,\n",
       "            1.4441e-07],\n",
       "           ...,\n",
       "           [2.7581e-07, 2.3902e-07, 1.0350e-07,  ..., 2.6469e-07, 1.3694e-07,\n",
       "            1.7952e-07],\n",
       "           [4.5971e-06, 3.1723e-06, 1.5607e-06,  ..., 3.4747e-06, 1.8668e-06,\n",
       "            2.3192e-06],\n",
       "           [3.5057e-06, 3.8542e-06, 2.3276e-06,  ..., 2.1109e-06, 2.5866e-06,\n",
       "            2.8156e-06]])},\n",
       "  17: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([[ 1.2195e-04,  2.3521e-04, -7.5940e-04,  ..., -1.0477e-05,\n",
       "            -2.4099e-04, -1.2715e-04],\n",
       "           [-1.0637e-04, -1.9140e-04,  4.6474e-04,  ..., -8.6100e-05,\n",
       "             9.3934e-05,  2.8143e-04],\n",
       "           [ 1.9465e-05, -1.1031e-04, -2.3153e-04,  ..., -3.4745e-06,\n",
       "            -1.4496e-04, -2.7508e-04],\n",
       "           ...,\n",
       "           [ 8.4313e-05,  6.4884e-05,  1.7553e-04,  ...,  2.5025e-05,\n",
       "             7.6314e-04, -4.6311e-04],\n",
       "           [ 1.9334e-05, -2.4956e-05, -6.4228e-05,  ..., -1.0854e-05,\n",
       "            -2.0639e-04, -7.6173e-05],\n",
       "           [ 1.9333e-05,  6.8059e-05,  3.4782e-04,  ...,  3.7384e-05,\n",
       "            -2.3522e-04,  3.8282e-05]]),\n",
       "   'exp_avg_sq': tensor([[1.1400e-06, 9.1071e-07, 1.7346e-06,  ..., 3.9621e-07, 4.8042e-06,\n",
       "            2.2017e-06],\n",
       "           [4.2018e-07, 1.3835e-06, 1.5994e-06,  ..., 4.4121e-07, 4.3019e-06,\n",
       "            2.1172e-06],\n",
       "           [3.2327e-07, 5.7499e-07, 1.0320e-06,  ..., 2.5454e-07, 2.1852e-06,\n",
       "            1.7338e-06],\n",
       "           ...,\n",
       "           [6.9300e-07, 1.1338e-06, 1.0436e-06,  ..., 3.4511e-07, 4.5180e-06,\n",
       "            1.5135e-06],\n",
       "           [5.1033e-07, 1.1376e-06, 1.2520e-06,  ..., 3.7047e-07, 3.2307e-06,\n",
       "            2.0878e-06],\n",
       "           [3.3874e-07, 9.4225e-07, 1.3257e-06,  ..., 3.5219e-07, 4.8005e-06,\n",
       "            1.9532e-06]])},\n",
       "  18: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([ 1.2616e-05, -1.0410e-03, -6.3513e-04,  2.3692e-04,  1.6204e-05,\n",
       "            1.4917e-04, -3.7462e-04, -8.6164e-04, -4.2640e-04,  1.8681e-06,\n",
       "           -7.9311e-04,  6.3351e-04, -1.7106e-04, -5.1234e-04,  2.7851e-05,\n",
       "           -4.8777e-04, -1.2385e-03, -6.6055e-04, -6.2489e-04,  4.2281e-04,\n",
       "            1.1268e-04, -1.6595e-04,  1.8212e-04,  1.0014e-04, -1.4754e-03,\n",
       "           -7.2804e-05, -1.0533e-04, -2.5541e-04, -1.4196e-03, -1.8217e-04,\n",
       "           -1.1043e-04,  1.4066e-04, -3.4859e-04, -2.3332e-04,  1.1064e-04,\n",
       "           -7.1888e-04, -1.7921e-04,  3.5123e-05, -5.3583e-05,  1.1799e-04,\n",
       "            1.2101e-06, -1.2994e-04, -2.7108e-04, -5.5909e-04,  1.3504e-04,\n",
       "           -1.0569e-04, -2.7980e-04, -5.6136e-04,  1.0591e-04, -5.7374e-04,\n",
       "           -1.4643e-05, -2.1895e-04,  2.2596e-04, -9.0930e-04, -2.3153e-05,\n",
       "           -1.2237e-04,  6.4297e-04, -1.0611e-03,  6.5595e-05,  4.4397e-04,\n",
       "           -5.0710e-05,  9.1844e-05,  5.4980e-05, -3.8596e-04]),\n",
       "   'exp_avg_sq': tensor([9.3175e-07, 8.7581e-06, 4.7836e-06, 2.3772e-06, 3.3616e-06, 2.1349e-06,\n",
       "           2.7627e-06, 3.7639e-06, 6.1837e-06, 2.3141e-06, 1.9628e-06, 2.3041e-06,\n",
       "           1.4493e-06, 8.5571e-06, 8.6364e-07, 2.4104e-06, 5.8701e-06, 5.1699e-06,\n",
       "           4.1247e-06, 4.9977e-06, 1.9447e-06, 3.5700e-06, 3.9725e-06, 2.1778e-06,\n",
       "           9.5551e-06, 1.3602e-06, 1.3768e-06, 4.9488e-06, 1.7676e-05, 2.4563e-06,\n",
       "           3.4854e-06, 4.4552e-06, 7.1383e-06, 1.5984e-06, 4.9646e-06, 1.2848e-06,\n",
       "           8.8655e-07, 2.1243e-06, 2.8699e-06, 1.5405e-06, 1.9247e-06, 1.5843e-06,\n",
       "           3.8756e-06, 2.3160e-06, 2.8588e-06, 1.6336e-06, 2.4516e-06, 6.2441e-06,\n",
       "           4.4505e-06, 2.9788e-06, 2.4849e-06, 1.2354e-06, 1.8392e-06, 1.2761e-05,\n",
       "           2.5294e-06, 1.1080e-06, 4.3047e-06, 4.0626e-06, 1.5388e-06, 2.4269e-06,\n",
       "           1.5849e-06, 1.4493e-06, 4.1324e-06, 5.6390e-06])},\n",
       "  19: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([-3.2013e-04,  8.0318e-04,  6.0093e-04, -2.2081e-03, -5.4084e-06,\n",
       "            5.7070e-04, -4.8173e-04,  1.1582e-03,  2.9089e-04,  8.6981e-05,\n",
       "           -5.8737e-04,  6.6312e-04,  7.2891e-04,  4.0572e-04, -1.5163e-03,\n",
       "            8.1988e-04,  5.4264e-04,  6.8542e-04, -2.1863e-04,  4.0657e-04,\n",
       "           -1.1761e-04, -2.5818e-04,  2.6400e-04,  2.7100e-04, -5.6529e-04,\n",
       "           -2.4653e-04,  7.9749e-04, -3.9215e-05, -6.8553e-04,  5.7640e-04,\n",
       "           -3.8836e-04,  6.1409e-04, -7.5184e-04, -8.2310e-04,  7.8377e-04,\n",
       "           -9.0152e-04,  1.5821e-05,  3.9069e-04, -8.7081e-04, -6.2659e-04,\n",
       "            3.0277e-04, -4.9460e-04, -3.2447e-04, -1.5100e-04,  1.8212e-04,\n",
       "           -1.2252e-04,  1.8929e-06, -5.0855e-05,  1.3073e-04,  7.7876e-04,\n",
       "            5.4037e-05, -1.0623e-03, -7.3441e-04, -6.2694e-04,  5.2222e-05,\n",
       "           -9.1158e-05,  6.2693e-04,  1.6705e-04, -1.6367e-04, -8.1380e-05,\n",
       "           -9.5350e-05, -7.5538e-04, -2.2566e-05,  3.9310e-04]),\n",
       "   'exp_avg_sq': tensor([2.4282e-06, 8.3505e-06, 4.4242e-06, 1.7148e-05, 1.8576e-06, 4.7048e-06,\n",
       "           2.2865e-06, 1.1469e-05, 3.0829e-06, 2.6737e-06, 8.0906e-06, 4.8388e-06,\n",
       "           6.4352e-06, 3.5460e-06, 1.2684e-05, 6.1012e-06, 7.6049e-06, 1.0318e-05,\n",
       "           2.2346e-06, 2.4323e-06, 3.3110e-06, 2.9138e-06, 5.2788e-06, 6.1222e-06,\n",
       "           1.1758e-05, 6.9108e-06, 4.3954e-06, 4.2146e-06, 7.8858e-06, 9.1376e-06,\n",
       "           2.0612e-06, 3.0186e-06, 1.3433e-05, 2.7912e-06, 4.3857e-06, 6.5703e-06,\n",
       "           1.9005e-06, 2.4614e-06, 3.6719e-06, 5.2139e-06, 2.8880e-06, 5.3725e-06,\n",
       "           4.0386e-06, 4.0459e-06, 1.1752e-05, 2.1256e-06, 2.2916e-06, 1.4118e-06,\n",
       "           3.1519e-06, 5.0033e-06, 6.3681e-06, 1.3716e-05, 5.2431e-06, 1.6440e-05,\n",
       "           3.8032e-06, 2.4858e-06, 3.4066e-06, 3.9022e-06, 2.3479e-06, 3.2516e-06,\n",
       "           4.7204e-06, 1.1232e-05, 3.1410e-06, 2.8544e-06])},\n",
       "  20: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([ 4.4939e-04, -1.9122e-04, -9.9220e-05, -6.5028e-05,  8.9578e-04,\n",
       "           -9.2030e-05,  1.3283e-04, -1.1781e-05, -6.9600e-04,  1.2363e-04,\n",
       "            4.6896e-05,  1.6702e-04, -2.2654e-04,  2.7818e-04, -3.2488e-04,\n",
       "           -1.5468e-04, -4.3335e-04,  4.6817e-04, -4.3108e-04, -2.3459e-04,\n",
       "            2.6480e-04,  3.5933e-06,  9.1258e-04, -2.1273e-04, -2.5799e-04,\n",
       "            2.8329e-04,  1.8111e-03, -4.0086e-04, -2.4510e-03, -5.6440e-05,\n",
       "           -2.3722e-05, -4.3584e-04, -8.9003e-04, -2.4581e-04, -5.3194e-04,\n",
       "           -8.6126e-05, -2.9317e-04,  1.0220e-03,  2.8740e-04,  1.5872e-04,\n",
       "           -5.2273e-05, -5.9954e-04, -3.5344e-04, -2.6254e-04, -4.3915e-05,\n",
       "           -7.4923e-06, -4.5889e-04, -4.5214e-04,  1.0911e-03, -1.0954e-04,\n",
       "           -5.5552e-05, -4.8463e-04,  1.7183e-04, -1.3887e-04,  4.8328e-05,\n",
       "            4.3747e-06,  4.0366e-04, -5.2468e-04,  5.2381e-04, -5.7909e-04,\n",
       "           -4.3265e-04,  3.9897e-04,  4.2148e-04, -1.2689e-05]),\n",
       "   'exp_avg_sq': tensor([1.9816e-06, 1.1278e-06, 2.8633e-06, 1.5752e-06, 1.2312e-05, 1.5244e-06,\n",
       "           1.8381e-06, 3.5535e-06, 2.8822e-06, 1.7300e-06, 1.4172e-06, 3.7699e-06,\n",
       "           6.7817e-07, 3.8087e-06, 1.5997e-06, 1.4069e-06, 2.8845e-06, 4.4898e-06,\n",
       "           2.4753e-06, 6.4620e-07, 2.2978e-06, 6.1595e-07, 8.3585e-06, 4.7868e-06,\n",
       "           1.7813e-06, 1.8291e-06, 2.4172e-05, 1.5478e-06, 2.7077e-05, 1.2005e-06,\n",
       "           9.6211e-06, 1.6689e-06, 7.6388e-06, 2.2733e-06, 1.5631e-06, 2.5051e-06,\n",
       "           1.5286e-06, 1.2657e-05, 2.1713e-06, 1.4045e-06, 8.4764e-07, 1.9024e-06,\n",
       "           1.5057e-06, 1.3701e-06, 9.0821e-07, 1.8727e-07, 1.7383e-06, 2.1755e-06,\n",
       "           7.5618e-06, 8.6733e-07, 3.4905e-06, 6.6633e-06, 4.0678e-06, 9.3282e-07,\n",
       "           1.8288e-06, 1.2676e-06, 4.1862e-06, 1.1654e-06, 3.7418e-06, 4.2335e-06,\n",
       "           8.2206e-07, 2.4293e-05, 2.1584e-06, 3.7823e-07])},\n",
       "  21: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([-6.5857e-04,  8.0513e-05,  1.2544e-05,  2.9656e-04,  6.1505e-05,\n",
       "            2.6689e-04, -8.0326e-05, -2.7749e-04,  1.4896e-04, -7.7668e-05,\n",
       "            3.5398e-04, -5.5392e-04, -3.1159e-04, -4.6975e-05,  2.6624e-04,\n",
       "           -8.2358e-05,  1.2405e-04, -3.6600e-05, -1.9786e-04, -3.9050e-06,\n",
       "            5.1055e-04, -7.9404e-06,  9.7983e-05, -1.5897e-05,  3.6164e-04,\n",
       "           -4.6707e-04,  3.4815e-04, -1.5594e-04,  3.5721e-04,  4.0776e-04,\n",
       "            1.4550e-04,  1.7396e-04, -4.0715e-04, -3.5796e-04, -6.6464e-05,\n",
       "            3.0677e-04, -3.2355e-04,  2.1122e-04,  1.0216e-04, -2.5557e-04,\n",
       "           -1.2409e-04, -4.1365e-04, -5.5546e-04,  1.0675e-04,  1.9130e-04,\n",
       "           -2.4700e-04,  1.4905e-04,  1.2406e-04,  1.5701e-04,  4.8750e-04,\n",
       "           -5.6866e-05, -5.4248e-05, -2.5499e-06, -2.5684e-05, -6.3029e-05,\n",
       "           -1.4545e-04, -3.5265e-05, -3.8102e-04,  9.4120e-05,  1.7208e-04,\n",
       "            2.6357e-04, -5.0184e-04, -1.8569e-04, -1.8867e-04]),\n",
       "   'exp_avg_sq': tensor([3.0810e-06, 1.7575e-06, 9.4226e-07, 5.9005e-06, 5.8034e-07, 8.0585e-07,\n",
       "           2.5796e-06, 1.6500e-06, 1.2379e-06, 8.9413e-07, 2.2586e-06, 1.9867e-06,\n",
       "           2.3400e-06, 8.1511e-07, 2.1128e-06, 9.9298e-07, 2.3639e-06, 3.0316e-06,\n",
       "           2.8022e-06, 6.4965e-07, 3.3644e-06, 2.6417e-06, 5.2197e-07, 6.8100e-06,\n",
       "           1.2642e-06, 2.7949e-06, 2.2904e-06, 5.2421e-07, 3.8099e-06, 1.5033e-06,\n",
       "           1.3965e-06, 1.5578e-06, 4.5170e-06, 9.5331e-07, 1.4408e-06, 2.1114e-06,\n",
       "           3.4467e-06, 1.0658e-06, 2.4396e-06, 8.8016e-07, 1.0859e-06, 2.7976e-06,\n",
       "           3.3367e-06, 1.0906e-06, 2.5165e-06, 3.3049e-06, 9.2270e-07, 1.2680e-06,\n",
       "           7.2900e-07, 3.4985e-06, 1.5879e-06, 1.7611e-06, 1.6526e-06, 2.6576e-06,\n",
       "           8.1159e-07, 2.3606e-06, 1.2064e-06, 1.6963e-06, 6.0016e-07, 3.2584e-06,\n",
       "           1.0648e-06, 5.0929e-06, 1.1090e-06, 5.8491e-07])},\n",
       "  22: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([-5.8626e-06, -4.4587e-04, -4.1894e-04, -4.0049e-04,  8.8027e-05,\n",
       "           -2.1996e-04, -3.5562e-04, -4.4417e-04, -1.1136e-04, -1.2442e-04,\n",
       "           -1.5310e-04, -1.0870e-04, -8.9409e-05,  5.7865e-05, -4.9517e-05,\n",
       "            7.2662e-05, -1.9528e-04, -2.9653e-06,  1.1895e-04,  3.9138e-06,\n",
       "           -6.7121e-06, -9.2243e-05, -9.5692e-05, -2.7100e-04, -5.7524e-04,\n",
       "           -8.3268e-05,  7.6902e-04,  5.3196e-05, -3.6336e-04, -2.2257e-04,\n",
       "           -1.4551e-04, -2.3413e-04, -3.3311e-04,  2.7069e-04, -2.2952e-04,\n",
       "           -3.2256e-04, -3.3628e-04,  4.0088e-04,  4.8631e-05, -6.1524e-05,\n",
       "           -2.1555e-04, -2.1459e-04, -4.4322e-05, -4.1029e-04, -7.8484e-05,\n",
       "            2.5259e-04, -2.8509e-04, -2.2464e-05,  1.7303e-05,  2.3558e-05,\n",
       "           -2.5559e-04, -5.4697e-04, -1.9586e-04, -3.6653e-04,  2.7673e-04,\n",
       "           -1.1341e-04, -2.0927e-04, -4.6570e-04,  1.2527e-04, -1.4110e-04,\n",
       "           -1.4418e-04, -2.6944e-04, -3.6597e-04, -2.3673e-04]),\n",
       "   'exp_avg_sq': tensor([2.5232e-07, 2.6960e-06, 6.3456e-07, 1.3016e-06, 3.8039e-07, 9.3746e-07,\n",
       "           5.1147e-06, 2.0492e-06, 1.7174e-06, 3.5884e-06, 2.3413e-07, 5.4495e-07,\n",
       "           1.5262e-06, 4.0854e-07, 4.9867e-07, 8.5739e-07, 8.7701e-07, 1.4788e-06,\n",
       "           6.6133e-07, 1.2399e-06, 6.6075e-07, 1.4740e-06, 5.4575e-07, 5.8588e-07,\n",
       "           2.2948e-06, 6.7628e-07, 5.9162e-06, 7.0130e-07, 9.7439e-07, 7.4897e-07,\n",
       "           4.9305e-07, 2.0531e-06, 8.8962e-07, 1.9060e-06, 4.4718e-07, 1.3688e-06,\n",
       "           9.3822e-07, 1.2028e-06, 5.5431e-07, 1.5638e-07, 9.5511e-07, 7.0426e-07,\n",
       "           1.2081e-06, 2.7684e-06, 2.8598e-07, 6.2674e-07, 1.9943e-06, 1.4855e-06,\n",
       "           5.5128e-07, 1.1405e-06, 1.1289e-06, 1.6450e-06, 6.5135e-07, 5.9511e-07,\n",
       "           1.1697e-06, 1.2328e-06, 8.3497e-07, 1.4425e-06, 4.8108e-07, 1.3213e-06,\n",
       "           5.0805e-07, 1.7544e-06, 1.2152e-06, 5.0048e-07])},\n",
       "  23: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([-2.2903e-04, -3.6426e-04,  1.5305e-04,  7.1476e-06, -1.8502e-04,\n",
       "           -2.2607e-04,  2.2345e-05, -1.3456e-05, -7.7564e-06, -1.4247e-05,\n",
       "           -9.3037e-05, -5.8744e-06,  3.9061e-05,  8.3051e-05,  9.7828e-06,\n",
       "            1.4777e-04, -4.7400e-04,  3.6830e-04, -3.3617e-04,  1.0268e-05,\n",
       "           -7.7734e-05, -2.2903e-05,  9.3366e-05, -6.6892e-05,  1.9066e-04,\n",
       "           -5.8671e-05, -1.9747e-04, -1.1026e-04, -4.7390e-05,  2.0371e-04,\n",
       "            3.7466e-04, -1.4588e-04, -2.3020e-05, -3.5894e-04,  1.3979e-04,\n",
       "            7.7252e-05,  1.1506e-04,  1.1426e-06, -2.6688e-04, -2.6327e-04,\n",
       "           -3.5357e-05, -3.0514e-04, -8.7541e-05,  1.0376e-04,  2.1953e-04,\n",
       "            9.9531e-05, -8.2021e-05, -1.2363e-04, -3.1008e-05,  2.8929e-04,\n",
       "           -1.1016e-04,  1.5744e-04, -2.8702e-05,  2.8196e-05,  1.7179e-05,\n",
       "            3.3394e-05, -4.9908e-05,  1.0750e-05,  1.0763e-04,  6.8101e-05,\n",
       "            6.4024e-05,  1.3964e-04, -3.2695e-04,  2.2417e-05]),\n",
       "   'exp_avg_sq': tensor([1.2147e-06, 6.6206e-07, 4.3523e-07, 2.2215e-06, 1.2513e-06, 2.4680e-07,\n",
       "           1.4176e-06, 1.2387e-06, 5.0594e-07, 7.9766e-07, 4.1861e-07, 2.0015e-07,\n",
       "           7.1841e-07, 9.4944e-07, 7.9911e-07, 1.1648e-06, 1.3513e-06, 8.4632e-07,\n",
       "           8.9213e-07, 2.1070e-06, 6.3403e-07, 5.7400e-07, 5.9082e-07, 1.2048e-06,\n",
       "           9.1286e-07, 1.5822e-06, 2.3336e-06, 4.8901e-07, 6.0759e-07, 9.3965e-07,\n",
       "           2.5591e-06, 6.0837e-07, 5.7586e-07, 1.4716e-06, 5.2442e-07, 1.1022e-06,\n",
       "           7.0848e-07, 2.7698e-06, 1.1266e-06, 5.3618e-07, 1.2239e-06, 1.2662e-06,\n",
       "           3.6533e-07, 7.5389e-07, 3.5115e-07, 1.1140e-06, 1.0230e-06, 1.0484e-06,\n",
       "           1.1455e-06, 1.2757e-06, 3.3156e-07, 9.1417e-07, 2.1132e-06, 5.3580e-07,\n",
       "           3.0932e-07, 1.0430e-06, 9.5106e-07, 4.4263e-07, 6.4196e-07, 7.6471e-07,\n",
       "           8.5641e-07, 5.8745e-07, 7.0807e-07, 5.0076e-07])},\n",
       "  24: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([-1.0437e-04,  1.8517e-04, -2.0626e-04, -8.4691e-05,  6.5089e-05,\n",
       "           -3.0049e-04, -7.0415e-04,  1.1351e-04,  2.8025e-05, -1.1070e-04,\n",
       "           -2.5417e-04, -4.3192e-05, -8.4303e-05, -3.1185e-04, -2.9482e-05,\n",
       "           -8.5671e-05, -3.6033e-04,  3.4036e-04, -1.3314e-04, -2.6698e-04,\n",
       "           -1.0368e-04, -3.2654e-04,  1.6887e-04,  1.2493e-04, -1.2422e-04,\n",
       "           -1.3120e-04, -2.1846e-04, -2.0027e-04, -1.8552e-04, -8.3558e-05,\n",
       "           -1.8756e-05,  1.2021e-04, -1.4697e-04,  9.5048e-04, -2.7191e-06,\n",
       "           -3.8718e-04, -2.0266e-05, -2.1087e-04,  2.7615e-05, -2.9982e-05,\n",
       "           -7.9548e-05, -2.9518e-04, -6.3081e-05,  1.4930e-04, -1.5840e-04,\n",
       "           -3.6843e-04, -2.3270e-04,  1.7722e-04, -2.9815e-06,  1.2805e-04,\n",
       "            1.3842e-04,  1.2194e-04, -3.2247e-04,  4.7682e-06, -2.8848e-05,\n",
       "           -1.6044e-05, -3.6301e-04,  2.1741e-05,  9.7907e-05, -6.4855e-05,\n",
       "           -2.5450e-05,  9.4765e-06,  3.1851e-04, -4.1201e-05]),\n",
       "   'exp_avg_sq': tensor([3.2780e-07, 1.8802e-06, 8.6906e-07, 4.9863e-07, 5.7269e-07, 5.8102e-07,\n",
       "           3.0536e-06, 1.5206e-06, 5.1773e-07, 9.1705e-07, 1.0622e-06, 4.8700e-07,\n",
       "           3.5426e-07, 5.9874e-07, 1.5347e-06, 1.6595e-06, 2.2596e-06, 1.6506e-06,\n",
       "           1.4823e-06, 7.2687e-07, 1.3809e-06, 3.7083e-07, 7.9969e-07, 8.3801e-07,\n",
       "           1.2488e-06, 1.2320e-06, 4.5845e-06, 6.3429e-07, 7.3873e-07, 5.4311e-07,\n",
       "           4.9765e-06, 1.1417e-06, 1.0484e-06, 3.1064e-06, 6.5519e-07, 5.1184e-07,\n",
       "           1.0086e-06, 9.2441e-07, 5.1399e-07, 5.3168e-07, 7.0886e-07, 1.1883e-06,\n",
       "           6.7364e-07, 1.7978e-06, 1.7216e-06, 2.2305e-06, 1.3067e-06, 1.6522e-06,\n",
       "           7.5691e-07, 1.2402e-06, 1.3733e-06, 1.2765e-06, 1.2490e-06, 6.6361e-07,\n",
       "           4.9143e-07, 5.1779e-07, 3.1844e-06, 6.6858e-07, 7.8432e-07, 1.1472e-06,\n",
       "           5.1879e-07, 3.2088e-06, 5.9567e-07, 7.6902e-07])},\n",
       "  25: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([ 5.0596e-05, -3.4621e-04,  8.2977e-05, -1.1082e-06, -4.2764e-04,\n",
       "           -1.0004e-04,  4.5574e-05, -3.4783e-04, -1.7551e-04, -1.1698e-04,\n",
       "           -1.1213e-04, -9.3145e-05,  7.8908e-06, -7.6317e-05, -2.7147e-04,\n",
       "           -7.3692e-06,  2.7060e-05,  4.6971e-05,  4.7428e-05, -1.0809e-04,\n",
       "           -7.0954e-05, -1.1446e-04, -2.4193e-04, -1.9634e-04, -2.8050e-04,\n",
       "           -5.1318e-04, -1.0384e-04, -1.2162e-04, -9.7634e-05, -2.1501e-06,\n",
       "           -2.5489e-04, -7.8511e-05,  1.8271e-04, -3.2680e-04, -2.3819e-04,\n",
       "           -3.7753e-04,  3.4540e-05, -1.4997e-04,  2.7455e-04, -7.3406e-05,\n",
       "           -1.5707e-04, -2.0151e-04, -2.3180e-04, -2.2071e-04,  1.8802e-04,\n",
       "            1.6470e-04, -1.7989e-04, -1.8085e-04, -3.0073e-04, -2.0601e-04,\n",
       "           -1.7563e-04, -3.2255e-05,  5.3407e-05, -5.4206e-05,  2.9591e-06,\n",
       "           -2.1358e-04, -3.0947e-05, -3.8085e-05, -2.5839e-04, -2.4751e-04,\n",
       "            1.3887e-04,  1.1991e-05,  1.8346e-04,  1.6059e-04]),\n",
       "   'exp_avg_sq': tensor([3.5212e-07, 5.8579e-07, 2.8578e-07, 6.1377e-07, 9.4875e-07, 5.0652e-07,\n",
       "           1.7615e-07, 5.8348e-07, 4.0612e-07, 6.8594e-07, 4.3722e-07, 2.5954e-07,\n",
       "           4.3393e-07, 2.8218e-07, 1.1642e-06, 8.5974e-07, 9.0251e-07, 9.5555e-07,\n",
       "           7.1829e-07, 6.1360e-07, 1.5314e-06, 3.8924e-07, 4.3353e-07, 3.7529e-07,\n",
       "           5.2847e-07, 8.9840e-07, 1.7765e-06, 2.0856e-07, 5.8876e-07, 5.9049e-07,\n",
       "           1.5996e-06, 4.5387e-07, 6.9839e-07, 1.2075e-06, 7.5409e-07, 1.0106e-06,\n",
       "           1.1851e-06, 5.1595e-07, 8.7394e-07, 1.4879e-07, 2.7407e-07, 7.3428e-07,\n",
       "           1.4128e-06, 5.4570e-07, 8.7216e-07, 2.9593e-07, 5.9144e-07, 4.5727e-07,\n",
       "           8.6350e-07, 8.6971e-07, 5.5031e-07, 7.3464e-07, 6.2410e-07, 2.0790e-07,\n",
       "           5.5735e-07, 4.8221e-07, 1.7158e-06, 4.0566e-07, 2.5492e-07, 3.4153e-07,\n",
       "           7.2155e-07, 5.4385e-07, 1.0104e-06, 6.2938e-07])},\n",
       "  26: {'step': tensor(1000.),\n",
       "   'exp_avg': tensor([-0.0011, -0.0014, -0.0011, -0.0012, -0.0006, -0.0006, -0.0009, -0.0006,\n",
       "           -0.0014, -0.0011, -0.0008, -0.0011, -0.0008, -0.0005, -0.0015, -0.0006,\n",
       "           -0.0008, -0.0011, -0.0013, -0.0014, -0.0005, -0.0003, -0.0011, -0.0008,\n",
       "           -0.0013, -0.0007, -0.0009, -0.0012, -0.0015, -0.0005, -0.0007, -0.0015,\n",
       "           -0.0016, -0.0012, -0.0009, -0.0004, -0.0010, -0.0011, -0.0012, -0.0009,\n",
       "           -0.0007, -0.0017, -0.0014, -0.0009, -0.0012, -0.0009, -0.0007, -0.0010,\n",
       "           -0.0007, -0.0010, -0.0009, -0.0014, -0.0010, -0.0012, -0.0009, -0.0008,\n",
       "           -0.0016, -0.0013, -0.0015, -0.0014, -0.0013, -0.0010, -0.0009, -0.0011]),\n",
       "   'exp_avg_sq': tensor([3.1635e-06, 2.8867e-06, 2.1772e-06, 2.0738e-06, 1.3987e-06, 9.2834e-07,\n",
       "           2.3728e-06, 1.7417e-06, 2.7773e-06, 5.1315e-06, 2.1040e-06, 2.7291e-06,\n",
       "           3.0414e-06, 1.6218e-06, 7.8064e-06, 3.6280e-06, 2.2663e-06, 3.8409e-06,\n",
       "           6.6496e-06, 3.0907e-06, 1.1261e-06, 5.1360e-07, 2.3636e-06, 1.7969e-06,\n",
       "           3.3664e-06, 3.7680e-06, 2.6815e-06, 3.6838e-06, 3.9058e-06, 1.6263e-06,\n",
       "           1.9395e-06, 3.9292e-06, 5.9456e-06, 3.5670e-06, 5.7931e-06, 1.5012e-06,\n",
       "           3.0612e-06, 2.5314e-06, 4.7747e-06, 2.2615e-06, 2.4569e-06, 3.6155e-06,\n",
       "           5.4069e-06, 2.3482e-06, 3.1989e-06, 2.1332e-06, 4.0496e-06, 1.6181e-06,\n",
       "           1.9599e-06, 2.4509e-06, 1.7364e-06, 5.0608e-06, 2.3855e-06, 2.8121e-06,\n",
       "           3.0528e-06, 1.3179e-06, 5.1644e-06, 2.4812e-06, 3.3491e-06, 3.2020e-06,\n",
       "           3.5449e-06, 3.3212e-06, 1.7113e-06, 2.7918e-06])}},\n",
       " 'param_groups': [{'weight_decay': 0.1,\n",
       "   'lr': 6e-05,\n",
       "   'betas': (0.9, 0.95),\n",
       "   'eps': 1e-08,\n",
       "   'amsgrad': False,\n",
       "   'foreach': None,\n",
       "   'maximize': False,\n",
       "   'capturable': False,\n",
       "   'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]},\n",
       "  {'weight_decay': 0.0,\n",
       "   'lr': 6e-05,\n",
       "   'betas': (0.9, 0.95),\n",
       "   'eps': 1e-08,\n",
       "   'amsgrad': False,\n",
       "   'foreach': None,\n",
       "   'maximize': False,\n",
       "   'capturable': False,\n",
       "   'params': [18, 19, 20, 21, 22, 23, 24, 25, 26]}]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b87e2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.20M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_model_args = checkpoint['model_args']\n",
    "model_args = dict()\n",
    "# force these config attributes to be equal otherwise we can't even resume training\n",
    "# the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "    model_args[k] = checkpoint_model_args[k]\n",
    "# create the model\n",
    "model = NanoGPTModel(GPTConfig(**model_args))\n",
    "state_dict = checkpoint['model']\n",
    "# fix the keys of the state dictionary\n",
    "# honestly no idea how checkpoints sometimes get this prefix. have to debug more\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed40d547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CAnde some:\n",
      "I not thich and furre anck, your whis of basted mored was borted brick suit mars indes,\n",
      "To a an day sthir, sthu\n",
      "Of orertst ham be the and warth depet.\n",
      "\n",
      "CENGARINIUS:\n",
      "WAy londe thens the berakest im so tham heath to fitht micke\n",
      "By of be homin, the har doott of\n",
      "An and base, o shive haven hand andel anste,\n",
      "Now sheal hared heam and by momere\n",
      "To a swarerting mare tour mute hy angond'ss sayse an song ton hivong.\n",
      "\n",
      "HANCEORD\n",
      "ARCK:\n",
      "TINGo sheas trie sthear, hepps, andears;\n",
      "To be sold you wou wher mate mirt.\n",
      "\n",
      "WICO:\n",
      "MAfark'Sd dind I to to the weell to han my by ankes\n",
      "dot mad, thou hild woou goment she theint\n",
      "Whe arrin atink dead.\n",
      "\n",
      "WINDUCES:\n",
      "Wird thet witth shin:\n",
      "Hou thish and ith is nour bue ot be hatt mert.\n",
      "\n",
      "INENTINTHIIUS:\n",
      "An ty surer thens beim may wavang,\n",
      "I hincer bimy sot atlle in as, ben thirence that the ave bof toou st angadse your soo ome donsh, sis owill\n",
      "An freaver, win atis theer mands thathild\n",
      "Is am ble have, a my so wesis:\n",
      "Burt hof hare the stim tho bate\n",
      "Wise of hatelly ashy berup ome the and:\n",
      "Ary tearte avel, ar thou tupen bue.\n",
      "\n",
      "CHAUKENI:\n",
      "INt you arer tair. sheing sheeas, ste to tire hy wat ta arss sincure;\n",
      "So ibe forcer.\n",
      "\n",
      "CHARUK:\n",
      "An I whys, sack.\n",
      "Wils bent and stherert.\n",
      "\n",
      "ARENGOO:\n",
      "A fold I beans till my thang there im of or a this theat is the word touth the theng a strong,\n",
      "And is heay stousee ar oas step,\n",
      "Thy ankerest, willd tounsir,\n",
      "I shell ongr tof\n",
      "The the arkencom a of andetir inc bene,\n",
      "Thiseng yeas whirkne beand sun ond\n",
      "To me hithth mond'll whe is hand him;\n",
      "Ancers is spantors and mank wherd brotth a and thelll: she fle my thoung:\n",
      "I\n",
      "Is not thim tho ondse shertong, tont thit:\n",
      "I anges, hof willen, this a sthurk him wald in weathe helll spreant for wace astrond; be he that ot oure beash, and your a frourdes.\n",
      "Ass I I me dint to woold, and hevest\n",
      "We so iver me somere thee for and sis hof is trowe itth fielrss want, you, deriors, hee mucks and of thall suse\n",
      "And blete thith bumine and wof sharte.\n",
      "\n",
      "ARTIEOS:\n",
      "Heass, werry, thouns werversou, hanks sto his\n",
      "I's h\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000, temperature=1.0, top_k=10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04a0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
