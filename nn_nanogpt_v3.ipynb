{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ef4f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFull definition of a GPT Language Model, all of it in this single file.\\nReferences:\\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\\n2) huggingface/transformers PyTorch implementation:\\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995564c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d67417d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Ci'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a897941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2764010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(text))))\n",
    "stoi = {c : i for i, c in enumerate(chars)}\n",
    "itos = {i : c for c, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da33f612",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n_train = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n_train]       # 90%\n",
    "eval_data = data[n_train:]         # 10%\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d02bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, block_size, batch_size, device_type, device):\n",
    "    # generate small batch of data of inputs data X and targets y\n",
    "    data = train_data if split == 'train' else eval_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    X = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        X, y = X.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7277fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias, PyTorch doesn't support simply bias=False\"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return F.layer_norm(X, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias) # 3 means key,query,value concatenate\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            #causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                         .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        B, T, C = X.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(X).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) @ (B, nh, hs, T) -> (B, nhs, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # output prjection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias) # in transformer paper, the dimension is 512 and projectino to 2048, so it's 4 times\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn. Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # projection the 4 times dimension back to dimension\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.c_fc(X)\n",
    "        X = self.gelu(X)\n",
    "        X = self.c_proj(X)\n",
    "        X = self.dropout(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X + self.attn(self.ln_1(X)) # + means residual connection\n",
    "        X = X + self.mlp(self.ln_2(X)) # + means residual connection\n",
    "        return X\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_embedding = nn.Embedding(config.vocab_size, config.n_embd), # (vocab_size, C)\n",
    "            position_embedding = nn.Embedding(config.block_size, config.n_embd), # (T, C)\n",
    "            dropout = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias), # final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.token_embedding.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply sepcial scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        \n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get substracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.position_embedding.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, X, y=None):\n",
    "        device = X.device\n",
    "        # X and y are both (B, T) tensor integers, B = batch_size, T = block_size\n",
    "        B, T = X.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape (T)\n",
    "        \n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.token_embedding(X) # (B, T, C)\n",
    "        pos_emb = self.transformer.position_embedding(pos) # (T, C)\n",
    "        X = self.transformer.dropout(tok_emb + pos_emb)\n",
    "        for block in self.transformer.blocks:\n",
    "            X = block(X) # (B, T, C)\n",
    "        X = self.transformer.ln_f(X)   # (B, T, C)\n",
    "        \n",
    "        if y is not None:\n",
    "            # if we are given some desired y also calculate the loss\n",
    "            logits = self.lm_head(X) # (B, T, vocab_size)\n",
    "#             B, T, C = logits.shape\n",
    "#             logits = logits.view(B*T, C)\n",
    "#             y = y.view(B*T)\n",
    "#             loss = F.cross_entropy(logits, y)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference time mini optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(X[:, [-1], :]) # note: using list[-1] to preserve the time dim\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.position_embedding.weight = nn.Parameter(self.transformer.position_embedding.weight[:block_size])\n",
    "        for block in self.transformer.blocks:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pertrained gpt: %s\" % model_type)\n",
    "        \n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':        dict(n_layer=12, n_head=12, n_embd=768), # 124M parameters\n",
    "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024), # 350M parameters\n",
    "            'gpt2-large':  dict(n_layer=36, n_head=20, n_embd=1280), # 774M parameters\n",
    "            'gpt2-x1':     dict(n_layer=48, n_head=15, n_embd=1600), # 1558M parameters\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "        \n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        \n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf) != {len(sd_keys)}}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # speecial treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model\n",
    "    \n",
    "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all the condidata parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorm don't\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer\n",
    "    \n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd // cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, X, temperature=1.0, top_k=None):\n",
    "        # X is (B, T) array of indices in the current context\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        # crop X to the last block_size tokens\n",
    "        X_cond = X if X.size(1) <= self.config.block_size else X[:, -self.config.block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits, loss = self(X_cond)\n",
    "        # focus only on the last time step, pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature # becomes (B, C)\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_nxt = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        X = torch.cat((X, idx_nxt), dim=1) # (B, T+1)\n",
    "        return X\n",
    "        \n",
    "    def generate(self, X, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices X (LongTensor of shape (B, T)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        # X is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            X = self.predict(X, temperature, top_k)\n",
    "        return X\n",
    "\n",
    "\n",
    "class GPT():\n",
    "    def __init__(self, vocab_size):\n",
    "        self._init_params(vocab_size)\n",
    "        \n",
    "        # model\n",
    "        model_args = dict(n_layer=self.n_layer, n_head=self.n_head, n_embd=self.n_embd, block_size=self.block_size,\n",
    "                 bias=self.bias, vocab_size=self.vocab_size, dropout=self.dropout)\n",
    "        self.model = GPTModel(GPTConfig(**model_args))\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # compile the model\n",
    "        if self.compile:\n",
    "            print(\"compiling the model... (take a ~minute)\")\n",
    "            self.unoptimized_model = self.model\n",
    "            self.model = torch.compile(self.model) # requires PyTorch >= 2.0\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = self.model.configure_optimizer(self.weight_decay, self.learning_rate,\n",
    "                                                        (self.beta1, self.beta2), self.device_type)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(self.eval_iters)\n",
    "            for k in range(self.eval_iters):\n",
    "                X, y = get_batch('train', self.block_size, self.batch_size, self.device_type, self.device)\n",
    "                logits, loss = self.model(X, y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        self.model.train()\n",
    "        return out\n",
    "\n",
    "    # learning rate decay scheduler (cosine with warmup)\n",
    "    def get_lr(self, it):\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < self.warmup_iters:\n",
    "            return self.learning_rate * it / self.warmup_iters\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > self.lr_decay_iters:\n",
    "            return self.min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "        return self.min_lr + coeff * (self.learning_rate - self.min_lr)\n",
    "    \n",
    "    def train(self):\n",
    "        # init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "        iter_num = 0\n",
    "        best_val_loss = 1e9\n",
    "        \n",
    "        # training loop\n",
    "        X, y = get_batch('train', self.block_size, self.batch_size, self.device_type, self.device) # fetch the very first batch\n",
    "        t0 = time.time()\n",
    "        local_iter_num = 0 # number of iterations in the lifetime of the process\n",
    "        running_mfu = -1.0\n",
    "        while True:\n",
    "            # determine and set the learning rate for this iteration\n",
    "            lr = self.get_lr(iter_num) if self.decay_lr else self.learning_rate\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            # evaluate the loss on train/eval sets and write checkpoints\n",
    "            if iter_num % self.eval_interval == 0:\n",
    "                losses = self.estimate_loss()\n",
    "                print(f\"step {iter_num}: train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "                if losses['eval'] < best_val_loss or self.always_save_checkpoint:\n",
    "                    best_val_loss = losses['eval']\n",
    "                    if iter_num > 0:\n",
    "                        checkpoint = {\n",
    "                            'model': self.model.state_dict(),\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'model_args': self.model.config,\n",
    "                            'iter_num': iter_num,\n",
    "                            'best_val_loss': best_val_loss,\n",
    "                        }\n",
    "                        print(f\"saving checkpoint to {self.out_dir}\")\n",
    "                        torch.save(checkpoint, os.path.join(self.out_dir, f'{self.model_name}_ckpt.pt'))\n",
    "            if iter_num == 0 and self.eval_only:\n",
    "                break\n",
    "\n",
    "            # forward backward update. with optional gradient accumulation to simulate larger batch size\n",
    "            # and using the GradScaler if data type is float16\n",
    "            for micro_step in range(self.gradient_accumulation_steps):\n",
    "                with self.ctx:\n",
    "                    logits, loss = self.model(X, y)\n",
    "                    loss = loss / self.gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "                # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "                X, y = get_batch('train', self.block_size, self.batch_size, self.device_type, self.device)\n",
    "                # backward pass, with gradient sacling if training in fp16\n",
    "                self.scaler.scale(loss).backward()\n",
    "            # clip the gradient\n",
    "            if self.grad_clip != 0.0:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "            # step the optimizer and scaler if training in fp16\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            # flush the gradients as soon as we can, no need for this memory anymore\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # timing and logging\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            t0 = t1\n",
    "            if iter_num % self.log_interval == 0:\n",
    "                # get loss as float. note: this is a CPU-GPU sync point\n",
    "                # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "                lossf = loss.item() * self.gradient_accumulation_steps\n",
    "                if local_iter_num >= 5: # let the training loop settle a bit\n",
    "                    mfu = self.model.estimate_mfu(self.batch_size * self.gradient_accumulation_steps, dt)\n",
    "                    running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "                print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "            iter_num += 1\n",
    "            local_iter_num += 1\n",
    "\n",
    "            # termination condition\n",
    "            if iter_num > self.epochs:\n",
    "                break\n",
    "        \n",
    "    def predict(self):\n",
    "        # generate from the model\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=self.device)\n",
    "        print(decode(self.model.generate(context, max_new_tokens=2000, temperature=1.0, top_k=10)[0].tolist()))\n",
    "    \n",
    "    def load(self):\n",
    "        # load saved checkpoint\n",
    "        checkpoint = torch.load(os.path.join(self.out_dir, f'{self.model_name}_ckpt.pt'), map_location=self.device)\n",
    "        model_args = checkpoint['model_args']\n",
    "        # create the model\n",
    "        self.model = GPTModel(model_args)\n",
    "        state_dict = checkpoint['model']\n",
    "        # fix the keys of the state dictionary\n",
    "        # honestly no idea how checkpoints sometimes get this prefix. have to debug more\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k,v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        checkpoint = None # free up memory\n",
    "\n",
    "        \n",
    "    def _init_params(self, vocab_size):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # I/O\n",
    "        self.out_dir = 'out'\n",
    "        self.model_name = 'nanogpt'\n",
    "        self.eval_interval = 100\n",
    "        self.log_interval = 100\n",
    "        self.eval_iters = 200\n",
    "        self.eval_only = False # if True, script exits right after the first eval\n",
    "        self.always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "       \n",
    "        # data\n",
    "        self.vocab_size = vocab_size\n",
    "        self.gradient_accumulation_steps = 3 # used to simulate larger batch sizes\n",
    "        self.batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "        #block_size = 1024\n",
    "        self.block_size = 32\n",
    "        \n",
    "        # model\n",
    "        self.n_layer = 4\n",
    "        self.n_head = 4\n",
    "        self.n_embd = 64\n",
    "        self.dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "        self.bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "        \n",
    "        # admw optimizer\n",
    "        self.learning_rate = 1e-3 # max leanring rate\n",
    "        self.epochs = 1000 # total number of training iterations\n",
    "        self.weight_decay = 1e-1\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.95\n",
    "        self.grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "        \n",
    "        # learning rate decay settings\n",
    "        self.decay_lr = True # whether to decay the learning rate\n",
    "        self.warmup_iters = 100 # how many steps to warm up for\n",
    "        self.lr_decay_iters = self.epochs # should be ~= epochs per Chinchilla\n",
    "        self.min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "        \n",
    "        # system\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "        self.device_type = 'cuda' if 'cuda' in self.device else 'cpu' # for later use in torch.autocast\n",
    "        self.dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16' or 'float16', the latter will auto implement a GradScaler\n",
    "        # note: float16 data type will automatically use a GradScaler\n",
    "        self.ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[self.dtype]\n",
    "        self.ctx = nullcontext() if self.device_type == 'cpu' else torch.amp.autocast(device_type=self.device_type, dtype=self.ptdtype)\n",
    "        self.compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "        \n",
    "        # torch\n",
    "        torch.manual_seed(1337)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "        torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "        # initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=(self.dtype == 'float16'))\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        tokens_per_iter = self.gradient_accumulation_steps * self.batch_size * self.block_size\n",
    "        print(f\"tokens per iteration will be {tokens_per_iter:,}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c26d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 32]), torch.Size([12, 32]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_batch('train', model.block_size, model.batch_size, model.device_type, model.device) # fetch the very first batch\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47817bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be 1,152\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.20M\n",
      "num decayed parameter tensors: 18, with 202,816 parameters\n",
      "num non-decayed parameter tensors: 9, with 576 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaoeason\\AppData\\Local\\anaconda3\\envs\\disk\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "model = GPT(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "827061d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1928, eval loss 4.1929\n",
      "iter 0: loss 4.1919, time 34361.78ms, mfu -100.00%\n",
      "step 100: train loss 2.8043, eval loss 2.7982\n",
      "saving checkpoint to out\n",
      "iter 100: loss 2.8130, time 35013.79ms, mfu 0.00%\n",
      "step 200: train loss 2.4909, eval loss 2.4981\n",
      "saving checkpoint to out\n",
      "iter 200: loss 2.5632, time 36243.09ms, mfu 0.00%\n",
      "step 300: train loss 2.4079, eval loss 2.4048\n",
      "saving checkpoint to out\n",
      "iter 300: loss 2.3739, time 37059.12ms, mfu 0.00%\n",
      "step 400: train loss 2.3489, eval loss 2.3464\n",
      "saving checkpoint to out\n",
      "iter 400: loss 2.3289, time 36166.40ms, mfu 0.00%\n",
      "step 500: train loss 2.2840, eval loss 2.2897\n",
      "saving checkpoint to out\n",
      "iter 500: loss 2.3265, time 35005.78ms, mfu 0.00%\n",
      "step 600: train loss 2.2383, eval loss 2.2349\n",
      "saving checkpoint to out\n",
      "iter 600: loss 2.3080, time 36029.30ms, mfu 0.00%\n",
      "step 700: train loss 2.2061, eval loss 2.2036\n",
      "saving checkpoint to out\n",
      "iter 700: loss 2.2699, time 35480.67ms, mfu 0.00%\n",
      "step 800: train loss 2.1737, eval loss 2.1775\n",
      "saving checkpoint to out\n",
      "iter 800: loss 2.1499, time 35795.70ms, mfu 0.00%\n",
      "step 900: train loss 2.1622, eval loss 2.1513\n",
      "saving checkpoint to out\n",
      "iter 900: loss 2.1202, time 35999.70ms, mfu 0.00%\n",
      "step 1000: train loss 2.1514, eval loss 2.1427\n",
      "saving checkpoint to out\n",
      "iter 1000: loss 2.1627, time 35754.97ms, mfu 0.00%\n"
     ]
    }
   ],
   "source": [
    "# load saved checkpoint\n",
    "#model.load()\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae6858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CARE:\n",
      "And I thy wathe thak'l wid that onds of and the friore; ith held thre mour weart brangesert to whall bede bows me that my ban hord\n",
      "Ang is berosoutiminn, to art o my bladde,\n",
      "Sough douene wertr, mord ton you,\n",
      "And fard thearest wol blotssen, you,\n",
      "An me them broth marknseseeles is him ar fror ies to thes.\n",
      "\n",
      "WOFongs, mord, she II serim tho he sount,\n",
      "Start amy thand mall or shy we houghalle this dou, surest you\n",
      "Th to dime tint and hend fearsse miort\n",
      "Bund her osof ba my ach brave imert anne ict brie, in forst a my.\n",
      "\n",
      "WOLORGLER:\n",
      "Hads trein thy, heils heave illlf han an otees if hernctesie for har the and by in sour, wigh should. I dover bookes wices and thet ind's mest o angeang fors, well mims on sticck,\n",
      "And thesurt st hould thou if tar sat, dases.\n",
      "\n",
      "TUTRIUSS:\n",
      "INo my sim whay, hilll we of hert.\n",
      "\n",
      "ACEERIAND-\n",
      "AOLOO:\n",
      "T:\n",
      "Therit iler if sor the meses sus savar ast ame, to must o his, shell won mat.\n",
      "\n",
      "ANESS:\n",
      "Bad I as thy livel.\n",
      "\n",
      "ARK:\n",
      "Nos here, and divinng tho as this for shate stas,\n",
      "Hat having:\n",
      "Th that it sunt, sans berourts of had deprovigher tim off tict siet horde\n",
      "And them seand splakess he theak.\n",
      "\n",
      "\n",
      "SRK:\n",
      "Serdfor'es an to dot thake if art heee\n",
      "He and sipllt to ont at bragientes.\n",
      "\n",
      "WORURELO:\n",
      "Band to hind mity tin ouncence.\n",
      "\n",
      "KILALIE say, wirll hof her blovere. yan\n",
      "\n",
      "CANENTO:\n",
      "An, murd thisst off or fowng theror fut and marssis, and hent ins tey tinkit, ing wong and shirs the the andsered ifort;\n",
      "Inds wor amps yourst mor of, soret the,\n",
      "Muth duch spriss ot ind wis a deashes thit. heat.\n",
      "\n",
      "\n",
      "\n",
      "GHKIUCEOUES:\n",
      "WA:\n",
      "Ses or tay it it sof beng oongis, whir of a mangond ourr breansing,\n",
      "And houg whe frrear, ist a mut fith\n",
      "Sane trir angaiety sthe berer if sthares\n",
      "Woull\n",
      "He tof ang shucch thim tesor onde mire a fling seeself ay stlee ives hend beat a to tourgar of-dneth thath t wim a andes sall thare.\n",
      "BERII:\n",
      "Hed, that dou by wordse to b things.\n",
      "\n",
      "HORDUD:\n",
      "Hut he hand that say ave, sthrrat.\n",
      "\n",
      "ANGENES:\n",
      "Warte mors to hom as of told thes welld,\n",
      "Shan of trour sor ant thay me but thas avous the swerray the wit\n"
     ]
    }
   ],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b04a0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.20M\n"
     ]
    }
   ],
   "source": [
    "model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3107ebc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CEE:\n",
      "By by ime to sey most foreser angass,\n",
      "Nour feavor, our witht, in the hup istordines shitis wa dortincind,\n",
      "I hof seald om ifur brod ind to a oo the to wath to o busionn wo she therre ish.\n",
      "\n",
      "\n",
      "LINERER:\n",
      "I, so, bure wheshas she ferret ank: mavest thour, anng, hereas son thoe tar avisghsse to hear thent is of beaine, the ang mang antellf sof ye ar meviden.\n",
      "\n",
      "CUURUS:\n",
      "LAn I wippplod you, wealld botie, tould, sherd youled mice my thannce thes topuler to tenow, thers sene,\n",
      "Ay frever of hom angring mong arest witlle,\n",
      "I mpre tansttor is whiss, in\n",
      "Te sist stinn wor of she the of\n",
      "Thalk: mort has of sa shenser\n",
      "Forstrars tend to to andicinge trenct,\n",
      "Ill warse ald anthim blit thear the ousth.\n",
      "\n",
      "MORUTETINO:\n",
      "Whir my to the thenst ath beeng sothe,\n",
      "Or this ipe arrviesh my this, to ance hang tloughthe is tours by housen trieted\n",
      "I havest stack tey and, an sterter our, wich tall,\n",
      "Ansir mperioong\n",
      "And where hist the tan moucink:\n",
      "Is that wim to she ond shen, thou,\n",
      "Im thish. helle, wist the that\n",
      "\n",
      "Is tis thave melorss thour ston dof twer,\n",
      "By lim arve thou have, thee tousts, to be int burnt the bee thas blace,\n",
      "Theld a thou beay in the a of thou het wall\n",
      "On to hache it weers brenong asirr thee an the soul bret oow\n",
      "Se dive mell.\n",
      "\n",
      "LLOLARINES sare you on my beat he whert mpient\n",
      "Ast thy tas our thy thoures ame bees forent. mest whou dorst berieteng.\n",
      "\n",
      "HENGIND\n",
      "IONCHES:\n",
      "Ne yall, belure of a four on ind thes in\n",
      "And mactong miting y othald a be that the\n",
      "I's we brand in ans thears ang aived the me warthisss alll and ton\n",
      "He to cond tharder, heer word itild marter the\n",
      "Felld mee, sthele a werrrese, ingart,\n",
      "Hyes brerietond this beling, a manck theavere too woh\n",
      "Thou mancee, be theall ysere and weres sead do tou hou.\n",
      "Taler bech seroff, I to lyy andserd,\n",
      "I war thate wid of shete, is ands a faced.\n",
      "\n",
      "Therdf tone and heve thes toparsitie of-\n",
      "Thorrt has aserst and aft and herd\n",
      "I fir beangs mangent yourt, wuld thouge ande then wat heat douse\n",
      "ad my saicre meray; be thour thave don the a thust\n",
      "Thell at suen to hee ting\n"
     ]
    }
   ],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4ea64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
