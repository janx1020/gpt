{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ef4f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFull definition of a GPT Language Model, all of it in this single file.\\nReferences:\\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\\n2) huggingface/transformers PyTorch implementation:\\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995564c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d67417d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Ci'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a897941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2764010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(text))))\n",
    "stoi = {c : i for i, c in enumerate(chars)}\n",
    "itos = {i : c for c, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da33f612",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394, 32]) torch.Size([1115394, 32])\n"
     ]
    }
   ],
   "source": [
    "block_size = 32\n",
    "\n",
    "def build_dataset(text):\n",
    "    X, y = [], []\n",
    "    context = [0] * block_size\n",
    "    for c in text:\n",
    "        #print(w)\n",
    "        X.append(context)\n",
    "        context = context[1:] + [stoi[c]]\n",
    "        y.append(context)\n",
    "        #print(f\"{''.join([itos[i] for i in context])} ==> {c}\")\n",
    "    #print(\"X:\", X, \"=>\", \" y:\", y)\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    print(X.shape, y.shape)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = build_dataset(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7277fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias, PyTorch doesn't support simply bias=False\"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return F.layer_norm(X, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias) # 3 means key,query,value concatenate\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            #causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                         .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        B, T, C = X.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(X).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) @ (B, nh, hs, T) -> (B, nhs, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # output prjection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias) # in transformer paper, the dimension is 512 and projectino to 2048, so it's 4 times\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn. Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # projection the 4 times dimension back to dimension\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.c_fc(X)\n",
    "        X = self.gelu(X)\n",
    "        X = self.c_proj(X)\n",
    "        X = self.dropout(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X + self.attn(self.ln_1(X)) # + means residual connection\n",
    "        X = X + self.mlp(self.ln_2(X)) # + means residual connection\n",
    "        return X\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_embedding = nn.Embedding(config.vocab_size, config.n_embd), # (vocab_size, C)\n",
    "            position_embedding = nn.Embedding(config.block_size, config.n_embd), # (T, C)\n",
    "            dropout = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias), # final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.token_embedding.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply sepcial scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        \n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get substracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.position_embedding.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, X, y=None):\n",
    "        device = X.device\n",
    "        # X and y are both (B, T) tensor integers, B = batch_size, T = block_size\n",
    "        B, T = X.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape (T)\n",
    "        \n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.token_embedding(X) # (B, T, C)\n",
    "        pos_emb = self.transformer.position_embedding(pos) # (T, C)\n",
    "        X = self.transformer.dropout(tok_emb + pos_emb)\n",
    "        for block in self.transformer.blocks:\n",
    "            X = block(X) # (B, T, C)\n",
    "        X = self.transformer.ln_f(X)   # (B, T, C)\n",
    "        \n",
    "        if y is not None:\n",
    "            # if we are given some desired y also calculate the loss\n",
    "            logits = self.lm_head(X) # (B, T, vocab_size)\n",
    "#             B, T, C = logits.shape\n",
    "#             logits = logits.view(B*T, C)\n",
    "#             y = y.view(B*T)\n",
    "#             loss = F.cross_entropy(logits, y)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference time mini optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(X[:, [-1], :]) # note: using list[-1] to preserve the time dim\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.position_embedding.weight = nn.Parameter(self.transformer.position_embedding.weight[:block_size])\n",
    "        for block in self.transformer.blocks:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pertrained gpt: %s\" % model_type)\n",
    "        \n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':        dict(n_layer=12, n_head=12, n_embd=768), # 124M parameters\n",
    "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024), # 350M parameters\n",
    "            'gpt2-large':  dict(n_layer=36, n_head=20, n_embd=1280), # 774M parameters\n",
    "            'gpt2-x1':     dict(n_layer=48, n_head=15, n_embd=1600), # 1558M parameters\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "        \n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        \n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf) != {len(sd_keys)}}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # speecial treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model\n",
    "    \n",
    "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all the condidata parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorm don't\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer\n",
    "    \n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd // cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, X, temperature=1.0, top_k=None):\n",
    "        # X is (B, T) array of indices in the current context\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        # crop X to the last block_size tokens\n",
    "        X_cond = X if X.size(1) <= self.config.block_size else X[:, -self.config.block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits, loss = self(X_cond)\n",
    "        # focus only on the last time step, pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature # becomes (B, C)\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_nxt = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        X = torch.cat((X, idx_nxt), dim=1) # (B, T+1)\n",
    "        return X\n",
    "        \n",
    "    def generate(self, X, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices X (LongTensor of shape (B, T)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        # X is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            X = self.predict(X, temperature, top_k)\n",
    "        return X\n",
    "\n",
    "\n",
    "class GPT():\n",
    "    def __init__(self, vocab_size, block_size):\n",
    "        self._init_params(vocab_size, block_size)\n",
    "        \n",
    "        # model\n",
    "        model_args = dict(n_layer=self.n_layer, n_head=self.n_head, n_embd=self.n_embd, block_size=self.block_size,\n",
    "                 bias=self.bias, vocab_size=self.vocab_size, dropout=self.dropout)\n",
    "        self.model = GPTModel(GPTConfig(**model_args))\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # compile the model\n",
    "        if self.compile:\n",
    "            print(\"compiling the model... (take a ~minute)\")\n",
    "            self.unoptimized_model = self.model\n",
    "            self.model = torch.compile(self.model) # requires PyTorch >= 2.0\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = self.model.configure_optimizer(self.weight_decay, self.learning_rate,\n",
    "                                                        (self.beta1, self.beta2), self.device_type)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(self.eval_iters)\n",
    "            for k in range(self.eval_iters):\n",
    "                X, y = self.get_batch(split)\n",
    "                logits, loss = self.model(X, y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        self.model.train()\n",
    "        return out\n",
    "\n",
    "    # learning rate decay scheduler (cosine with warmup)\n",
    "    def get_lr(self, it):\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < self.warmup_iters:\n",
    "            return self.learning_rate * it / self.warmup_iters\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > self.lr_decay_iters:\n",
    "            return self.min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "        return self.min_lr + coeff * (self.learning_rate - self.min_lr)\n",
    "    \n",
    "    def get_batch(self, split):\n",
    "        # generate small batch of data of inputs data X and targets y\n",
    "        X, y = (self.X_train, self.y_train) if split == 'train' else (self.X_eval, self.y_eval)\n",
    "        ix = torch.randint(X.shape[0] - self.block_size, (self.batch_size,))\n",
    "        X = X[ix]\n",
    "        y = y[ix]\n",
    "        if self.device_type == 'cuda':\n",
    "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            X, y = X.pin_memory().to(self.device, non_blocking=True), y.pin_memory().to(self.device, non_blocking=True)\n",
    "        else:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "        return X, y\n",
    "    \n",
    "    def dataset(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "        assert X.shape[0] == y.shape[0], f\"X count {X.shape[0]} != y count {y.shape[0]}\"\n",
    "        n = X.shape[0]\n",
    "        n_train = int(n * 0.9)\n",
    "        self.X_train, self.X_eval = self.X.split(n_train) \n",
    "        self.y_train, self.y_eval = self.y.split(n_train) \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.dataset(X, y)\n",
    "        \n",
    "        best_val_loss = 1e9\n",
    "        \n",
    "        # training loop\n",
    "        X, y = self.get_batch('train') # fetch the very first batch\n",
    "        t0 = time.time()\n",
    "        running_mfu = -1.0\n",
    "        for iter_num in range(self.epochs):\n",
    "            # determine and set the learning rate for this iteration\n",
    "            lr = self.get_lr(iter_num) if self.decay_lr else self.learning_rate\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            # evaluate the loss on train/eval sets and write checkpoints\n",
    "            if iter_num % self.eval_interval == 0:\n",
    "                losses = self.estimate_loss()\n",
    "                print(f\"step {iter_num}: train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "                if losses['eval'] < best_val_loss or self.always_save_checkpoint:\n",
    "                    best_val_loss = losses['eval']\n",
    "                    if iter_num > 0:\n",
    "                        checkpoint = {\n",
    "                            'model': self.model.state_dict(),\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'model_args': self.model.config,\n",
    "                            'iter_num': iter_num,\n",
    "                            'best_val_loss': best_val_loss,\n",
    "                        }\n",
    "                        print(f\"saving checkpoint to {self.out_dir}\")\n",
    "                        torch.save(checkpoint, os.path.join(self.out_dir, f'{self.model_name}_ckpt.pt'))\n",
    "            if iter_num == 0 and self.eval_only:\n",
    "                break\n",
    "\n",
    "            # forward backward update. with optional gradient accumulation to simulate larger batch size\n",
    "            # and using the GradScaler if data type is float16\n",
    "            for micro_step in range(self.gradient_accumulation_steps):\n",
    "                with self.ctx:\n",
    "                    logits, loss = self.model(X, y)\n",
    "                    loss = loss / self.gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "                # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "                X, y = self.get_batch('train')\n",
    "                # backward pass, with gradient sacling if training in fp16\n",
    "                self.scaler.scale(loss).backward()\n",
    "            # clip the gradient\n",
    "            if self.grad_clip != 0.0:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "            # step the optimizer and scaler if training in fp16\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            # flush the gradients as soon as we can, no need for this memory anymore\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # timing and logging\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            t0 = t1\n",
    "            if iter_num % self.log_interval == 0:\n",
    "                # get loss as float. note: this is a CPU-GPU sync point\n",
    "                # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "                lossf = loss.item() * self.gradient_accumulation_steps\n",
    "                if iter_num >= 5: # let the training loop settle a bit\n",
    "                    mfu = self.model.estimate_mfu(self.batch_size * self.gradient_accumulation_steps, dt)\n",
    "                    running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "                print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "        \n",
    "    def predict(self):\n",
    "        # generate from the model\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=self.device)\n",
    "        print(decode(self.model.generate(context, max_new_tokens=2000, temperature=1.0, top_k=10)[0].tolist()))\n",
    "    \n",
    "    def load(self):\n",
    "        # load saved checkpoint\n",
    "        checkpoint = torch.load(os.path.join(self.out_dir, f'{self.model_name}_ckpt.pt'), map_location=self.device)\n",
    "        model_args = checkpoint['model_args']\n",
    "        # create the model\n",
    "        self.model = GPTModel(model_args)\n",
    "        state_dict = checkpoint['model']\n",
    "        # fix the keys of the state dictionary\n",
    "        # honestly no idea how checkpoints sometimes get this prefix. have to debug more\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k,v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        checkpoint = None # free up memory\n",
    "\n",
    "        \n",
    "    def _init_params(self, vocab_size, block_size):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # I/O\n",
    "        self.out_dir = 'out'\n",
    "        self.model_name = 'nanogpt'\n",
    "        self.eval_interval = 100\n",
    "        self.log_interval = 100\n",
    "        self.eval_iters = 200\n",
    "        self.eval_only = False # if True, script exits right after the first eval\n",
    "        self.always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "       \n",
    "        # data\n",
    "        self.vocab_size = vocab_size\n",
    "        self.gradient_accumulation_steps = 3 # used to simulate larger batch sizes\n",
    "        self.batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # model\n",
    "        self.n_layer = 4\n",
    "        self.n_head = 4\n",
    "        self.n_embd = 64\n",
    "        self.dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "        self.bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "        \n",
    "        # admw optimizer\n",
    "        self.learning_rate = 1e-3 # max leanring rate\n",
    "        self.epochs = 1000 # total number of training iterations\n",
    "        self.weight_decay = 1e-1\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.95\n",
    "        self.grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "        \n",
    "        # learning rate decay settings\n",
    "        self.decay_lr = True # whether to decay the learning rate\n",
    "        self.warmup_iters = 100 # how many steps to warm up for\n",
    "        self.lr_decay_iters = self.epochs # should be ~= epochs per Chinchilla\n",
    "        self.min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "        \n",
    "        # system\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "        self.device_type = 'cuda' if 'cuda' in self.device else 'cpu' # for later use in torch.autocast\n",
    "        self.dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16' or 'float16', the latter will auto implement a GradScaler\n",
    "        # note: float16 data type will automatically use a GradScaler\n",
    "        self.ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[self.dtype]\n",
    "        self.ctx = nullcontext() if self.device_type == 'cpu' else torch.amp.autocast(device_type=self.device_type, dtype=self.ptdtype)\n",
    "        self.compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "        \n",
    "        # torch\n",
    "        torch.manual_seed(1337)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "        torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "        # initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=(self.dtype == 'float16'))\n",
    "        #--------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        tokens_per_iter = self.gradient_accumulation_steps * self.batch_size * self.block_size\n",
    "        print(f\"tokens per iteration will be {tokens_per_iter:,}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47817bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be 1,152\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.20M\n",
      "num decayed parameter tensors: 18, with 202,816 parameters\n",
      "num non-decayed parameter tensors: 9, with 576 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaoeason\\AppData\\Local\\anaconda3\\envs\\disk\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "model = GPT(vocab_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "827061d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1916, eval loss 4.1894\n",
      "iter 0: loss 4.2010, time 31011.92ms, mfu -100.00%\n",
      "step 100: train loss 2.7949, eval loss 2.8036\n",
      "saving checkpoint to out\n",
      "iter 100: loss 2.7320, time 35269.19ms, mfu 0.00%\n",
      "step 200: train loss 2.4751, eval loss 2.4751\n",
      "saving checkpoint to out\n",
      "iter 200: loss 2.4963, time 31466.05ms, mfu 0.00%\n",
      "step 300: train loss 2.3930, eval loss 2.3959\n",
      "saving checkpoint to out\n",
      "iter 300: loss 2.4460, time 32351.11ms, mfu 0.00%\n",
      "step 400: train loss 2.3344, eval loss 2.3301\n",
      "saving checkpoint to out\n",
      "iter 400: loss 2.4796, time 34102.48ms, mfu 0.00%\n",
      "step 500: train loss 2.2794, eval loss 2.2739\n",
      "saving checkpoint to out\n",
      "iter 500: loss 2.2458, time 33444.76ms, mfu 0.00%\n",
      "step 600: train loss 2.2474, eval loss 2.2394\n",
      "saving checkpoint to out\n",
      "iter 600: loss 2.2949, time 32756.46ms, mfu 0.00%\n",
      "step 700: train loss 2.1891, eval loss 2.2059\n",
      "saving checkpoint to out\n",
      "iter 700: loss 2.1917, time 31748.46ms, mfu 0.00%\n",
      "step 800: train loss 2.1711, eval loss 2.1799\n",
      "saving checkpoint to out\n",
      "iter 800: loss 2.2062, time 33090.69ms, mfu 0.00%\n",
      "step 900: train loss 2.1456, eval loss 2.1610\n",
      "saving checkpoint to out\n",
      "iter 900: loss 2.0865, time 33614.24ms, mfu 0.00%\n"
     ]
    }
   ],
   "source": [
    "# load saved checkpoint\n",
    "#model.load()\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae6858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mett yor to an my, the opath on a whall.\n",
      "\n",
      "Shave wiss merith you:\n",
      "These a blis ankst ardentinteres, and macce.\n",
      "\n",
      "HORIV:\n",
      "Shour ary;\n",
      "An berstsould afor wis notie, is sut hes met wit wis o the of dlooogh,\n",
      "Thou is staind the tis she in hating is on theren int of incars\n",
      "Twhooult, ostorsten hortelirse as old ind merrse ices mach matincer.\n",
      "\n",
      "ALRO:\n",
      "I you, duth ba dring, this oust sate,\n",
      "Horting bof may his is to sthe dusers ford.\n",
      "Frot, the hom ther sty hoord thavell.\n",
      "And ble hen arristes to nomel hir will dentee deavests.\n",
      "\n",
      "MINE brace, my I:\n",
      "Shen to a andesss in ore he to a bre tam oncatest.\n",
      "And whay yoou nand if howst whill my sean mun itht the oue,\n",
      "A mand aith the tacar hont you wome taich housts.\n",
      "\n",
      "HARES:\n",
      "Mid'lll that shis tary!\n",
      "\n",
      "Thalt ind hath a the thou sor sely she\n",
      "the a and to a but shompen to ande mof seeese:\n",
      "Anclon heree hir meice.\n",
      "\n",
      "LENLING that I:\n",
      "ORu hell yreath to thall ane toess\n",
      "Whee dear ou sas ind shan bunkelle dit.\n",
      "Antolll I you hof to thande thhe mest our orour a fan a mate tho soll ourting'ss, at wald,\n",
      "Hat, bace my sie bon is mat ards aind with hengaint, at be wit.\n",
      "\n",
      "Sis him this thou sthou if she mesay;\n",
      "Mavery that of, is thind sich han the murt hof his dome ose. hon is tourt,\n",
      "Shind this havisest.\n",
      "\n",
      "NENCLASe, IE:\n",
      "Sis my becong; here thou horr is welll.\n",
      "My, so wee an am heaves andintseang,\n",
      "I weing the in mear ait hat moming't so in thise by werats houn,\n",
      "Mald hat ofure, of a anent youe our han hourd dind?\n",
      "\n",
      "TOULIE:\n",
      "CENoon so heak hing sait is heald, for saicg-be the that ine bisty ith man are,\n",
      "Buer the foor the heall weat, is went.\n",
      "\n",
      "Whoust to mete itild suar sof thist\n",
      "Thow henoure heree dowse twe wach oulld an in imone.\n",
      "\n",
      "AROCHELEO:\n",
      "Hat this I's you dighe wo waplt he mot\n",
      "And that sut sthee bears dow igelt annes hit\n",
      "Theat sontst your a be of bence betes?\n",
      "\n",
      "OLORENGRDIII:\n",
      "St I me thincars youe are dour thou hy toutht,\n",
      "The he wite be ath of machen she of houst\n",
      "Tind soo anst, thy martise stoe mare mas stour hackinn\n",
      "The heis ond thelay's wealll:\n",
      "As your wourd and hastou\n"
     ]
    }
   ],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b04a0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.20M\n"
     ]
    }
   ],
   "source": [
    "model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3107ebc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I su shis in and aimy;\n",
      "An the soond you, me thon doudst in os on a the besod.\n",
      "Cloud I's hove do thing\n",
      "II, thu the bristak besent, to ist alle an hap trey allourd,\n",
      "Thall and tond tund ourt a hims sis;\n",
      "Thivest thert yould an ot of then\n",
      "I minkin to we toines de sthese, in to hich it wist inst.\n",
      "\n",
      "Sas won me ICULA:\n",
      "Ay sir nond ble isch oushe, thes.\n",
      "\n",
      "LUSTUS:\n",
      "LI Iell therach sain hat maind isin thou dessten\n",
      "I houpld minch, to bof my thee\n",
      "And bre mearen sa wisen trit, buth sto stinde,\n",
      "Whe one son, hit thas ord sor me bet, hest,--dreay\n",
      "Hith imeer ta selom tur a ath osting\n",
      "Byot oun, on slorer with sourd thee all with wall arting\n",
      "Anged seenn ipesos ancere, shie ich\n",
      "As inchy wir to mante, hens sesp your sore wach,\n",
      "On in outh heves hist shersit.\n",
      "\n",
      "\n",
      "TINERD:\n",
      "Lok mord. the theseldss a that ind'ld you a bullt\n",
      "WI whall wind maverties wit hat tos is mave,\n",
      "To mand and byouters hon wure to bat if hee\n",
      "Mame thow a ta thes trow;\n",
      "To sorget tat may, ofult, mavight are whilles and your thit a oll a my heeld.\n",
      "\n",
      "TARADANGELTR:\n",
      "Bourtht Inot ich is what of ofse mastiellinss:\n",
      "Anown of a sthere arde tiin:\n",
      "O benchy too it. wis alld marder murd\n",
      "A his hamong: hive my there hesear matid omert,\n",
      "Wharmeang bust anst your nat saimt.\n",
      "\n",
      "LENES:\n",
      "Shen mard wis and in orn mord were theare:\n",
      "\n",
      "And hond whethe to the thase bent tandie\n",
      "Tin strof\n",
      "Ase hent whe heeme avald dhee sacortstine, harse haplavere he hasst hown sproth mere,\n",
      "Iserours trat thow, she thonge ont thoust ish ith\n",
      "I heame on ithes our and, whathe to owirst\n",
      "The wor bect hom thy mearse herut aflon.\n",
      "Favhim bendstat.\n",
      "\n",
      "LART:\n",
      "Cande mell we shares ast sere me thee wou as sale to's\n",
      "To wit mell moug he owsh daith blese,\n",
      "Whatitilf here thou be winth sharve hrence wit.\n",
      "\n",
      "SIONCLILIUS:\n",
      "As you me ord dost me salll it to thas thou\n",
      "On andeinde inst if funcicten wot.\n",
      "\n",
      "ACKEORIILO:\n",
      "I do you herechoue himer hith mald. ha thath burtstint is has tou the bove owe owilt.\n",
      "Ast bith stor to hest becht, youll she whou the hous thome thist\n",
      "Tange wou mase tho sour with there weit\n",
      "The doo\n"
     ]
    }
   ],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7f6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
