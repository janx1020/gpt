{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995564c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d67417d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Ci'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a897941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2764010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(text))))\n",
    "stoi = {c : i for i, c in enumerate(chars)}\n",
    "itos = {i : c for c, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da33f612",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n_train = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n_train]       # 90%\n",
    "eval_data = data[n_train:]         # 10%\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "633ff87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "epochs = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feda93f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2192e211d90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d02bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate small batch of data of inputs data X and targets y\n",
    "    data = train_data if split == 'train' else eval_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    X = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58317582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[59, 56, 43, 42,  1, 51, 43,  1, 57, 39, 63,  1, 51, 63,  1, 51, 47, 52,\n",
       "          42,  6,  0, 13, 52, 42,  1, 47, 44,  1, 63, 53, 59,  1],\n",
       "         [47, 58,  1, 52, 53, 58,  1, 58, 46, 43, 52,  1, 53, 59, 56,  1, 43, 63,\n",
       "          43, 50, 47, 42, 57,  1, 57, 47, 52, 49, 12,  1, 21,  1]]),\n",
       " tensor([[56, 43, 42,  1, 51, 43,  1, 57, 39, 63,  1, 51, 63,  1, 51, 47, 52, 42,\n",
       "           6,  0, 13, 52, 42,  1, 47, 44,  1, 63, 53, 59,  1, 41],\n",
       "         [58,  1, 52, 53, 58,  1, 58, 46, 43, 52,  1, 53, 59, 56,  1, 43, 63, 43,\n",
       "          50, 47, 42, 57,  1, 57, 47, 52, 49, 12,  1, 21,  1, 44]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_batch('eval')\n",
    "X[:2], y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd59e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, y = get_batch(split)\n",
    "            logits, loss = model(X, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7277fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # input size: (batch, time-step, channels) == (batch_size, block_size, n_embd)\n",
    "        # output size: (batch, time-step, head size) == (batch_size, block_size, head_size)\n",
    "        B, T, C = X.shape\n",
    "        k = self.key(X)   # (B, T, hs)\n",
    "        q = self.query(X) # (B, T, hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] **-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(X) # (B, T, hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-atttention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = torch.cat([h(X) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linerity\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), # in transformer paper, the dimension is 512 and projectino to 2048, so it's 4 times\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: commnunication followed by computation\"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X + self.sa(self.ln1(X))\n",
    "        X = X + self.ffwd(self.ln2(X))\n",
    "        return X\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)    # (vocab_size, C)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # (T, C)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        # better init, important\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, X, y=None):\n",
    "        # X and y are both (B, T) tensor integers, B = batch_size, T = block_size\n",
    "        B, T = X.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(X) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        X = tok_emb + pos_emb\n",
    "        X = self.blocks(X) # (B, T, C)\n",
    "        X = self.ln_f(X)   # (B, T, C)\n",
    "        logits = self.lm_head(X) # (B, T, vocab_size)\n",
    "        \n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, X, max_new_tokens):\n",
    "        # X is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop X to the last block_size tokens\n",
    "            X_cond = X[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(X_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_nxt = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            X = torch.cat((X, idx_nxt), dim=1) # (B, T+1)\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51353f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee55d4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters number: 209729\n"
     ]
    }
   ],
   "source": [
    "print('parameters number:', sum(p.nelement() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e4f188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1405, eval loss 4.1400\n",
      "step 100: train loss 2.6153, eval loss 2.6084\n",
      "step 200: train loss 2.4567, eval loss 2.4561\n",
      "step 300: train loss 2.3705, eval loss 2.3839\n",
      "step 400: train loss 2.3026, eval loss 2.3068\n",
      "step 500: train loss 2.2183, eval loss 2.2404\n",
      "step 600: train loss 2.1790, eval loss 2.2249\n",
      "step 700: train loss 2.1254, eval loss 2.1574\n",
      "step 800: train loss 2.0916, eval loss 2.1378\n",
      "step 900: train loss 2.0478, eval loss 2.1056\n",
      "step 1000: train loss 2.0408, eval loss 2.0891\n",
      "step 1100: train loss 1.9999, eval loss 2.0652\n",
      "step 1200: train loss 1.9740, eval loss 2.0666\n",
      "step 1300: train loss 1.9613, eval loss 2.0449\n",
      "step 1400: train loss 1.9421, eval loss 2.0409\n",
      "step 1500: train loss 1.9028, eval loss 2.0020\n",
      "step 1600: train loss 1.8864, eval loss 1.9970\n",
      "step 1700: train loss 1.8835, eval loss 1.9921\n",
      "step 1800: train loss 1.8862, eval loss 2.0023\n",
      "step 1900: train loss 1.8473, eval loss 1.9790\n",
      "step 2000: train loss 1.8518, eval loss 1.9652\n",
      "step 2100: train loss 1.8360, eval loss 1.9732\n",
      "step 2200: train loss 1.8268, eval loss 1.9569\n",
      "step 2300: train loss 1.8006, eval loss 1.9408\n",
      "step 2400: train loss 1.7875, eval loss 1.9265\n",
      "step 2500: train loss 1.7784, eval loss 1.9188\n",
      "step 2600: train loss 1.7639, eval loss 1.9034\n",
      "step 2700: train loss 1.7642, eval loss 1.9065\n",
      "step 2800: train loss 1.7554, eval loss 1.9042\n",
      "step 2900: train loss 1.7434, eval loss 1.8893\n",
      "step 3000: train loss 1.7452, eval loss 1.8911\n",
      "step 3100: train loss 1.7326, eval loss 1.8803\n",
      "step 3200: train loss 1.7276, eval loss 1.8856\n",
      "step 3300: train loss 1.7234, eval loss 1.8707\n",
      "step 3400: train loss 1.7184, eval loss 1.8749\n",
      "step 3500: train loss 1.7099, eval loss 1.8612\n",
      "step 3600: train loss 1.7117, eval loss 1.8488\n",
      "step 3700: train loss 1.6959, eval loss 1.8503\n",
      "step 3800: train loss 1.7010, eval loss 1.8540\n",
      "step 3900: train loss 1.6937, eval loss 1.8463\n",
      "step 4000: train loss 1.6800, eval loss 1.8229\n",
      "step 4100: train loss 1.6724, eval loss 1.8441\n",
      "step 4200: train loss 1.6714, eval loss 1.8420\n",
      "step 4300: train loss 1.6761, eval loss 1.8227\n",
      "step 4400: train loss 1.6728, eval loss 1.8433\n",
      "step 4500: train loss 1.6742, eval loss 1.8300\n",
      "step 4600: train loss 1.6596, eval loss 1.8332\n",
      "step 4700: train loss 1.6616, eval loss 1.8138\n",
      "step 4800: train loss 1.6553, eval loss 1.8235\n",
      "step 4900: train loss 1.6489, eval loss 1.8191\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "for iter in range(epochs):\n",
    "    # every once in a while evaluate the loss on train and eval sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    Xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(Xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47817bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "May him I vows their dispice-with sedwain your and\n",
      "Not mid loruniven, awas yet a nixite,\n",
      "Ay aid, to whath; me seach mad this me?\n",
      "Slay you have onten sent.\n",
      "\n",
      "KI ING HENRY G ROK:\n",
      "What's that a whed no migh Ad Justries post:\n",
      "O ducceday you hold shall be country in the dire of Payullain:\n",
      "Nay, you suir ad; now, bole\n",
      "Shall that then youm crie, and lass; lady? 'I would must not much oar.\n",
      "\n",
      "HISCWISTAnges Nambling of sutingly you, whethere intemond heaven?\n",
      "\n",
      "PETOLYCUS:\n",
      "Nay, and thee kill\n",
      "Who we prering sepeak then for of airnot\n",
      "Hath, Unnce, 'fulty is?\n",
      "\n",
      "First Must SERMIOW:\n",
      "I abaginage, I by must that and the\n",
      "stooke fainst gives to that.\n",
      "\n",
      "VISTIS:\n",
      "For, if you a pure that shamAes.\n",
      "Oxted oflowerly I land him this shall seaus; say I came postigervet\n",
      "That the moticke to run has offerit,\n",
      "This than this it moore; shall heart again' deadfore\n",
      "To Richanting to consta, and yas\n",
      "And I have ownd faulty of ouch no Edways reasong:\n",
      "This themself, to of hear. Wolloow yong; where tell footh?\n",
      "\n",
      "\n",
      "MARCIUS:\n",
      "thouse my paring too?\n",
      "\n",
      "Duch I was again;\n",
      "The wombles no earr me and my lostmach\n",
      "Overy?\n",
      "\n",
      "CLEOMERS:\n",
      "That i' roposes in gormas frint:\n",
      "Exchand and to head for our inseche workling yet me as worl, though I kinch, as do\n",
      "void my turealy thou wtherip unfatulestimons, and I moother,\n",
      "Dearished yourth hath as mock; you brother is i' for constrain.\n",
      "\n",
      "ANGELO:\n",
      "Buse, but you'll mened cannot so lights;\n",
      "That how, upon angrainst you him love slaugive not:\n",
      "My his dues; and reather adoh obeembrows and have\n",
      "swe the carchampedest Like he, if Brucking spukpring,\n",
      "For And corrant wall bleges the joiin.\n",
      "\n",
      "CLAUDIO:\n",
      "Not gueth iside thither for I thou and mades of driyngh:\n",
      "Ving of the bringble restabote!\n",
      "Tell a we, it the namopble upon the you is the Joliver.\n",
      "\n",
      "CORIOLANUS:\n",
      "Ay, I scousils. That had my gaves goot,\n",
      "That give saw, my trainems in lorn\n",
      "Hear Clominesss agive twith brickance healt\n",
      "This lift yet are again, indear some amostaine of arscrest\n",
      "I awas yet dombse the row stith quake!\n",
      "Freetnger thirs art waking son lifer and my ha\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b244ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
