{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ef4f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFull definition of a GPT Language Model, all of it in this single file.\\nReferences:\\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\\n2) huggingface/transformers PyTorch implementation:\\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995564c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d67417d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Ci'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a897941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2764010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(text))))\n",
    "stoi = {c : i for i, c in enumerate(chars)}\n",
    "itos = {i : c for c, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da33f612",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n_train = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n_train]       # 90%\n",
    "eval_data = data[n_train:]         # 10%\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633ff87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 32\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "epochs = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feda93f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x210ae9e2270>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d02bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate small batch of data of inputs data X and targets y\n",
    "    data = train_data if split == 'train' else eval_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    X = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58317582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[59, 56, 43, 42,  1, 51, 43,  1, 57, 39, 63,  1, 51, 63,  1, 51, 47, 52,\n",
       "          42,  6,  0, 13, 52, 42,  1, 47, 44,  1, 63, 53, 59,  1],\n",
       "         [47, 58,  1, 52, 53, 58,  1, 58, 46, 43, 52,  1, 53, 59, 56,  1, 43, 63,\n",
       "          43, 50, 47, 42, 57,  1, 57, 47, 52, 49, 12,  1, 21,  1]]),\n",
       " tensor([[56, 43, 42,  1, 51, 43,  1, 57, 39, 63,  1, 51, 63,  1, 51, 47, 52, 42,\n",
       "           6,  0, 13, 52, 42,  1, 47, 44,  1, 63, 53, 59,  1, 41],\n",
       "         [58,  1, 52, 53, 58,  1, 58, 46, 43, 52,  1, 53, 59, 56,  1, 43, 63, 43,\n",
       "          50, 47, 42, 57,  1, 57, 47, 52, 49, 12,  1, 21,  1, 44]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_batch('eval')\n",
    "X[:2], y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd59e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, y = get_batch(split)\n",
    "            logits, loss = model(X, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7277fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias, PyTorch doesn't support simply bias=False\"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return F.layer_norm(X, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "#     block_size: int = 1024\n",
    "#     vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "#     n_layer: int = 12\n",
    "#     n_head: int = 12\n",
    "#     n_embd: int = 768\n",
    "#     dropout: float = 0.0\n",
    "#     bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    block_size: int = block_size\n",
    "    vocab_size: int = vocab_size\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 64\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = False # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias) # 3 means key,query,value concatenate\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            #causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                         .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        B, T, C = X.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(X).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) @ (B, nh, hs, T) -> (B, nhs, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # output prjection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias) # in transformer paper, the dimension is 512 and projectino to 2048, so it's 4 times\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn. Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # projection the 4 times dimension back to dimension\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.c_fc(X)\n",
    "        X = self.gelu(X)\n",
    "        X = self.c_proj(X)\n",
    "        X = self.dropout(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X + self.attn(self.ln_1(X)) # + means residual connection\n",
    "        X = X + self.mlp(self.ln_2(X)) # + means residual connection\n",
    "        return X\n",
    "\n",
    "\n",
    "class NanoGPTLanguageModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_embedding = nn.Embedding(config.vocab_size, config.n_embd), # (vocab_size, C)\n",
    "            position_embedding = nn.Embedding(config.block_size, config.n_embd), # (T, C)\n",
    "            dropout = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias), # final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.token_embedding.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "        \n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply sepcial scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        \n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get substracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.position_embedding.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, X, y=None):\n",
    "        device = X.device\n",
    "        # X and y are both (B, T) tensor integers, B = batch_size, T = block_size\n",
    "        B, T = X.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape (T)\n",
    "        \n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.token_embedding(X) # (B, T, C)\n",
    "        pos_emb = self.transformer.position_embedding(pos) # (T, C)\n",
    "        X = self.transformer.dropout(tok_emb + pos_emb)\n",
    "        for block in self.transformer.blocks:\n",
    "            X = block(X) # (B, T, C)\n",
    "        X = self.transformer.ln_f(X)   # (B, T, C)\n",
    "        \n",
    "        if y is not None:\n",
    "            # if we are given some desired y also calculate the loss\n",
    "            logits = self.lm_head(X) # (B, T, vocab_size)\n",
    "#             B, T, C = logits.shape\n",
    "#             logits = logits.view(B*T, C)\n",
    "#             y = y.view(B*T)\n",
    "#             loss = F.cross_entropy(logits, y)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference time mini optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(X[:, [-1], :]) # note: using list[-1] to preserve the time dim\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, X, max_new_tokens):\n",
    "        # X is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop X to the last block_size tokens\n",
    "            X_cond = X[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(X_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_nxt = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            X = torch.cat((X, idx_nxt), dim=1) # (B, T+1)\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51353f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.20M\n"
     ]
    }
   ],
   "source": [
    "model = NanoGPTLanguageModel(GPTConfig())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee55d4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters number: 203392\n"
     ]
    }
   ],
   "source": [
    "print('total parameters number:', sum(p.nelement() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74e4f188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1844, eval loss 4.1840\n",
      "step 100: train loss 2.6713, eval loss 2.6584\n",
      "step 200: train loss 2.5072, eval loss 2.5084\n",
      "step 300: train loss 2.4233, eval loss 2.4314\n",
      "step 400: train loss 2.3848, eval loss 2.3849\n",
      "step 500: train loss 2.3422, eval loss 2.3411\n",
      "step 600: train loss 2.2891, eval loss 2.2915\n",
      "step 700: train loss 2.2536, eval loss 2.2576\n",
      "step 800: train loss 2.2422, eval loss 2.2439\n",
      "step 900: train loss 2.1927, eval loss 2.2089\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "for iter in range(epochs):\n",
    "    # every once in a while evaluate the loss on train and eval sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    Xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(Xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47817bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ESIRY IY:\n",
      "Paveng he-Kard see honen Ixen hangs of-Zends,\n",
      "Ind brou, the sour ing me, thit die.\n",
      "\n",
      "\n",
      "PUtom habeth, your grasijar'nzen say the hat your coung oof wand\n",
      "thablk : slises frushe. I sicot of orbe hefblt theat, larde ther hiply't hit ich berer. wighwesss a math\n",
      "Fou, li.\n",
      "\n",
      "Hupast an Rice?\n",
      "TRICMOLITAM'NENYIORIUK:\n",
      "Kenfaits.\n",
      "\n",
      "Sighn Rusm'd I to abuts I det the denmearss\n",
      "My, fakvivengrn's our mon he may with ridey kish ing thish is--\n",
      "Bndbuost homurghy, you haghting ath, cowaull twer wackeden renst.\n",
      "\n",
      "NORUKulan:\n",
      "Nay, the Vakest hekoks nazitt mellbhiste.\n",
      "\n",
      "HEFLOLEEY:\n",
      "The ise thassen bly, a youu poll atim'?\n",
      "\n",
      "KENprure Thats igh:\n",
      "Hhavll oum the trofe of wethy taiths duse ayAns thene to ean sucay dise,\n",
      "JOrversuknk groikesiong stomrpient atin'sIn tupeeld baet; I loche omartore; ikn, wis ton prostevear, growerst con, hand;\n",
      "Fell I kneam salll: be'bllt to aegrimant\n",
      "Soimt nour hanmf supeom with, Aglaed\n",
      "Shang, bous ay buartong obrs; her squiken be,\n",
      "Arused the but the the cut my you ournwanch unell',\n",
      "Farss, Welllan, the of keak of rirsth\n",
      "Somam ton pae chaved toup Kuches hat wownsooursD hast sine,\n",
      "Why tou sutllor combller Vandyen\n",
      "owitplonc silby layinked sond itse\n",
      "be iast go my hellal aber akegue ky kisens to bedsts,\n",
      "Fay hilt borle ditke speaps eice!\n",
      "Weds nerakef precck x-knanfse.e ustend?\n",
      "\n",
      "ORAMUSIULE:\n",
      "I marikesthn tish 'Then pit, mes ice borst, birs ofon;\n",
      "So an\n",
      "Tary karlts my wolll bume? riist hearsen thaull no\n",
      "Mattend dy'erncbh touschn senoy, verd Orido;\n",
      "Fineks a-ble of toll lotuske, gand,\n",
      "And yeak neath lou tst thelle he haverse.\n",
      "\n",
      "WROWENIONCINTHYANIRD:\n",
      "WaceTh, bristgnef, ur, kretwrmensts of hardr aft ensise\n",
      "In boarion, stof in In deoe, whaclly thes of fors I Pomors? beall.\n",
      "\n",
      "\n",
      "ICESTIUUWIVH:\n",
      "KE? st stichnden\n",
      "Ifor bowik In cake, I brill then'sey:\n",
      "Ou thas himon Inebur ceave chear bontumesener, uth the thos heard;\n",
      "Abre; yheeat rinxespermevinay,\n",
      "Whailll and thise of langef quors thate fplord, thell.\n",
      "\n",
      "DUke, stst.\n",
      "ARGOBUKEED:\n",
      "K:\n",
      "SinconinndnnWit.\n",
      "\n",
      "Vasend wond am the Kaive in suy trencpemonaay\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b244ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
